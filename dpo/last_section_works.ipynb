{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13214831,"sourceType":"datasetVersion","datasetId":8375858},{"sourceId":13214895,"sourceType":"datasetVersion","datasetId":8375911},{"sourceId":13215065,"sourceType":"datasetVersion","datasetId":8376037},{"sourceId":594025,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444583,"modelId":461071}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pdb\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        if self.flash:\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None, return_hidden_states=False, full_seq=False):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        elif full_seq:\n            logits = self.lm_head(x)\n            loss = None\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            loss = None\n        \n        if return_hidden_states:\n            return logits, loss, x\n        else:\n            return logits, loss\n\n    def crop_block_size(self, block_size):\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {}\n\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257\n        config_args['block_size'] = 1024\n        config_args['bias'] = True\n\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        flops_achieved = flops_per_iter * (1.0/dt)\n        flops_promised = 312e12\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _, hidden_state = self(idx_cond,return_hidden_states=True)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            if idx_next.item()==0:\n                break   \n            if idx_next.item()==7:\n                idx = torch.cat((idx, idx_next), dim=1)\n                break\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx, hidden_state\n\nclass GPTRewardModel(nn.Module):\n    def __init__(self, gpt):\n        super().__init__()\n        self.gpt = gpt\n        self.value_head = nn.Sequential(\n            nn.Linear(self.gpt.config.n_embd, self.gpt.config.n_embd),\n            nn.ReLU(),\n            nn.Linear(self.gpt.config.n_embd, 1)\n        )\n\n    def forward(self, input_ids):\n        _, _, hidden_states = self.gpt(input_ids, return_hidden_states=True)\n        mask = (input_ids != 0).unsqueeze(-1)\n        masked_hidden = hidden_states * mask\n        sum_hidden = masked_hidden.sum(dim=1)\n        lengths = mask.sum(dim=1)\n        pooled = sum_hidden / lengths.clamp(min=1)\n        logits = self.value_head(pooled).squeeze(-1)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:10:30.709072Z","iopub.execute_input":"2025-10-09T04:10:30.709290Z","iopub.status.idle":"2025-10-09T04:10:37.308686Z","shell.execute_reply.started":"2025-10-09T04:10:30.709272Z","shell.execute_reply":"2025-10-09T04:10:37.307921Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:44:52.834066Z","iopub.execute_input":"2025-09-30T07:44:52.834419Z","iopub.status.idle":"2025-09-30T07:44:55.544585Z","shell.execute_reply.started":"2025-09-30T07:44:52.834366Z","shell.execute_reply":"2025-09-30T07:44:55.543637Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Step 1: Install necessary packages","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib\n!pip install torch numpy transformers datasets tiktoken wandb tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:11:10.835063Z","iopub.execute_input":"2025-10-09T04:11:10.835797Z","iopub.status.idle":"2025-10-09T04:12:41.324201Z","shell.execute_reply.started":"2025-10-09T04:11:10.835771Z","shell.execute_reply":"2025-10-09T04:12:41.323276Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Step 2: Package imports and configuration","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n# tokenizer\n\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s]\ndef decode(l): return ''.join([itos[i] for i in l])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:13:45.200322Z","iopub.execute_input":"2025-10-09T04:13:45.200986Z","iopub.status.idle":"2025-10-09T04:13:45.284041Z","shell.execute_reply.started":"2025-10-09T04:13:45.200937Z","shell.execute_reply":"2025-10-09T04:13:45.283246Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pickle\n\n# Load the original meta.pkl (from your GPT training or /kaggle/input path)\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\n\nstoi = meta[\"stoi\"]\nitos = meta[\"itos\"]\n\n# Print out how big the vocab is\nprint(\"Vocab size:\", len(stoi))\n\n# Show the first 50 characters (sorted for readability)\nprint(\"Sample of stoi keys (characters):\")\nprint(sorted(list(stoi.keys()))[:50])\n\n# If you want to see ALL characters the model supports:\nprint(\"All tokens in original vocab:\")\nprint(sorted(stoi.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:13:47.423455Z","iopub.execute_input":"2025-10-09T04:13:47.424129Z","iopub.status.idle":"2025-10-09T04:13:47.429965Z","shell.execute_reply.started":"2025-10-09T04:13:47.424104Z","shell.execute_reply":"2025-10-09T04:13:47.429365Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 74\nSample of stoi keys (characters):\n['\\n', ' ', \"'\", '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c']\nAll tokens in original vocab:\n['\\n', ' ', \"'\", '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '’']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Step 3: Define helper functions","metadata":{}},{"cell_type":"code","source":"def compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    #for l in lines:\n    #    print(l[1])\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:13:50.813996Z","iopub.execute_input":"2025-10-09T04:13:50.814742Z","iopub.status.idle":"2025-10-09T04:13:50.822142Z","shell.execute_reply.started":"2025-10-09T04:13:50.814716Z","shell.execute_reply":"2025-10-09T04:13:50.821335Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Step 4: Load the pretrained NanoGPT model","metadata":{}},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\n# Load policy model\ngpt = GPT(gptconf)\ngpt.load_state_dict(state_dict)\n# IMPORTANT: re-tie token embedding and lm_head weights after load\ngpt.lm_head.weight = gpt.transformer.wte.weight\ngpt.to(device).train()\n\n# Load frozen reference\nref_gpt = GPT(gptconf)\nref_gpt.load_state_dict(state_dict)\n# IMPORTANT: same tying on reference\nref_gpt.lm_head.weight = ref_gpt.transformer.wte.weight\nref_gpt.to(device).eval()\nfor p in ref_gpt.parameters():\n    p.requires_grad = False\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:41:44.885270Z","iopub.execute_input":"2025-10-09T04:41:44.886032Z","iopub.status.idle":"2025-10-09T04:41:45.369242Z","shell.execute_reply.started":"2025-10-09T04:41:44.886008Z","shell.execute_reply":"2025-10-09T04:41:45.368440Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"gpt.eval(); ref_gpt.eval()\nx = torch.tensor(encode(\"12*8=? The answer is 96 because 12*8 equals 96.\\n\"), dtype=torch.long, device=device)[None, :-1]\nwith torch.no_grad():\n    lg_pt, _ = gpt(x, full_seq=True)\n    lg_rf, _ = ref_gpt(x, full_seq=True)\nprint(\"Max abs diff pre-train:\", (lg_pt - lg_rf).abs().max().item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:41:46.599763Z","iopub.execute_input":"2025-10-09T04:41:46.600024Z","iopub.status.idle":"2025-10-09T04:41:46.613242Z","shell.execute_reply.started":"2025-10-09T04:41:46.600002Z","shell.execute_reply":"2025-10-09T04:41:46.612463Z"}},"outputs":[{"name":"stdout","text":"Max abs diff pre-train: 0.0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Step 5: Load Data \n","metadata":{}},{"cell_type":"code","source":"# Load data from ./data/pos_neg_pairs.json\n\n# Loading the json file, CHANGE ADDRESS IF NEEDED\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n    raw_data = json.load(f)\n\nallowed_chars = set(stoi.keys())\n\ndef clean_text(text, allowed):\n    return \"\".join([c if c in allowed else \".\" for c in text])\n    \n# Clean the dataset\nlines = [{\"negative\": clean_text(e[\"negative\"], allowed_chars),\n          \"positive\": clean_text(e[\"positive\"], allowed_chars)} for e in raw_data]\n\nprint(f\"Loaded {len(lines)} pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:41:57.439571Z","iopub.execute_input":"2025-10-09T04:41:57.440090Z","iopub.status.idle":"2025-10-09T04:41:58.082095Z","shell.execute_reply.started":"2025-10-09T04:41:57.440068Z","shell.execute_reply":"2025-10-09T04:41:58.081310Z"}},"outputs":[{"name":"stdout","text":"Loaded 100000 pairs.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Step 6: Build the optimizer and scheduler","metadata":{}},{"cell_type":"code","source":"import math\n\n# Configuration \ngradient_accumulation_steps = 8  # Simulate larger batch sizes\nweight_decay = 0.01 # UPDATED\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nwarmup_iters = 200\nmax_iters = (len(lines) // batch_size) * epochs\nlr_decay_iters = max_iters\nmin_lr = base_lr / 10\n\n# Mixed precision setup \ndtype = 'float16' if torch.cuda.is_available() else 'float32'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == 'cuda' else torch.no_grad()\n\n# Initialize GradScaler\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# Optimizer \noptimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\n\n# Learning rate decay scheduler (cosine with warmup) \ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return base_lr * (it + 1) / (warmup_iters + 1)\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n    return min_lr + coeff * (base_lr - min_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:42:09.417673Z","iopub.execute_input":"2025-10-09T04:42:09.417937Z","iopub.status.idle":"2025-10-09T04:42:09.426525Z","shell.execute_reply.started":"2025-10-09T04:42:09.417916Z","shell.execute_reply":"2025-10-09T04:42:09.425706Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1109073563.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Step 7: Begin training","metadata":{}},{"cell_type":"code","source":"# new compute logprob\nbeta = 0.2  # gentler than 0.5\n\ndef compute_policy_ref_logprob(input_ids, policy_model, ref_model):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n\n    policy_logits, _ = policy_model(inputs, full_seq=True)\n    with torch.no_grad():\n        ref_logits, _ = ref_model(inputs, full_seq=True)\n\n    policy_logp = F.log_softmax(policy_logits, dim=-1)\n    ref_logp = F.log_softmax(ref_logits, dim=-1)\n\n    tgt_pol = policy_logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n    tgt_ref = ref_logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n\n    mask = (targets != 0).float()\n    denom = mask.sum(dim=1).clamp(min=1)\n\n    seq_pol = (tgt_pol * mask).sum(dim=1) / denom\n    seq_ref = (tgt_ref * mask).sum(dim=1) / denom\n    return seq_pol, seq_ref","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:42:13.171020Z","iopub.execute_input":"2025-10-09T04:42:13.171724Z","iopub.status.idle":"2025-10-09T04:42:13.179313Z","shell.execute_reply.started":"2025-10-09T04:42:13.171689Z","shell.execute_reply":"2025-10-09T04:42:13.178621Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"total_steps = len(lines) // batch_size\niter_num = 0  # Global iteration counter\nt0 = time.time()  # Timing\n\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n        ###########################################################\n        # Standard training step \n\n        # Dynamic learning rate (cosine decay with warmup)\n        lr = get_lr(iter_num) if decay_lr else base_lr\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Forward + loss (mixed precision)\n        with ctx:\n            #neg_logprob = compute_logprob(neg_tensor)\n            #pos_logprob = compute_logprob(pos_tensor)\n\n            # Direct Preference Optimization (DPO) loss\n            #loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n\n            # UPDATED: Use reference model for DPO\n            pos_pol, pos_ref = compute_policy_ref_logprob(pos_tensor, gpt, ref_gpt)\n            neg_pol, neg_ref = compute_policy_ref_logprob(neg_tensor, gpt, ref_gpt)\n            dpo_arg = (pos_pol - pos_ref) - (neg_pol - neg_ref)\n            loss = -F.logsigmoid(dpo_arg / beta).mean()\n\n        # Backward (with gradient scaling if fp16/bf16 mode)\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        if grad_clip != 0.0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n\n        # Optimizer + scaler step\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Reset gradients (\"flush\" them to save memory)\n        optimizer.zero_grad(set_to_none=True)\n\n        # Timing + logging\n        t1 = time.time()\n        dt = t1 - t0\n        t0 = t1\n\n        lossf = loss.item()  # no rescaling needed\n\n        pbar.set_description(\n            f\"Epoch {epoch+1} iter {iter_num} loss {lossf:.4f} lr {lr:.2e} time {dt*1000:.1f}ms\"\n        )\n\n        iter_num += 1\n        ###########################################################\n\n    ckpt_path = f\"./dpo.pt\"\n    torch.save({\n        \"model_state_dict\": gpt.state_dict(),\n        \"model_args\": ckpt['model_args'],\n    }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:42:14.716192Z","iopub.execute_input":"2025-10-09T04:42:14.716820Z","iopub.status.idle":"2025-10-09T04:56:32.976999Z","shell.execute_reply.started":"2025-10-09T04:42:14.716789Z","shell.execute_reply":"2025-10-09T04:56:32.976199Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 iter 1561 loss 0.0000 lr 9.31e-05 time 110.1ms: : 1562it [02:52,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 iter 3123 loss 0.0000 lr 7.10e-05 time 109.4ms: : 1562it [02:51,  9.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 iter 4685 loss 0.0000 lr 4.25e-05 time 108.4ms: : 1562it [02:51,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 iter 6247 loss 0.0000 lr 1.90e-05 time 109.5ms: : 1562it [02:51,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 iter 7809 loss 0.0000 lr 1.00e-05 time 110.3ms: : 1562it [02:51,  9.12it/s]","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Step 8: Begin testing","metadata":{}},{"cell_type":"code","source":"# Test\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\"]\n\nwith torch.no_grad():\n    for prompt in test_set:\n        # Encode text → tensor\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n\n        # Generate continuation\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        # Convert back to text\n        generated_tokens = out[0][0].cpu().tolist()\n\n        # Split into prompt + continuation\n        prompt_len = len(prompt_ids)\n        full_text = decode(generated_tokens)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T04:56:43.039375Z","iopub.execute_input":"2025-10-09T04:56:43.040082Z","iopub.status.idle":"2025-10-09T04:56:43.457101Z","shell.execute_reply.started":"2025-10-09T04:56:43.040046Z","shell.execute_reply":"2025-10-09T04:56:43.456370Z"}},"outputs":[{"name":"stdout","text":"Prompt: 17+19=?\nAnswer: Yes, I have a dog\n\nPrompt: 3*17=?\nAnswer: Yes, I ke an us, take an umbrella\n\nPrompt: 72/4=?\nAnswer: Yes, I have\n\nPrompt: 72-x=34,x=?\nAnswer: I don’s ke away\n\nPrompt: x*11=44,x=?\nAnswer: I me mes ove Yes, I ke an us -Yes, take an umbrella\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig  # FIXED: Uncomment this line!\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\nimport math\n\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length = 64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n\n# tokenizer\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s]\ndef decode(l): return ''.join([itos[i] for i in l])\n\ndef compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor\n\n# Load model\nckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\ngpt = GPT(gptconf)\nstate_dict = ckpt['model']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\ngpt.to(device).train()\n\n# FIXED: Better data loading and cleaning\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding=\"utf-8\") as f:\n    raw_data = json.load(f)\n\nallowed_chars = set(stoi.keys())\n\ndef clean_text(text, allowed):\n    # FIXED: Instead of replacing with '.', skip unknown characters or use a space\n    return \"\".join([c if c in allowed else ' ' for c in text])\n\n# Clean the dataset\nlines = []\nfor entry in raw_data:\n    neg_clean = clean_text(entry[\"negative\"], allowed_chars)\n    pos_clean = clean_text(entry[\"positive\"], allowed_chars)\n    \n    # FIXED: Only add if both texts are reasonable length after cleaning\n    if len(neg_clean.strip()) > 5 and len(pos_clean.strip()) > 5:\n        lines.append({\n            \"negative\": neg_clean,\n            \"positive\": pos_clean\n        })\n\nprint(f\"Loaded {len(lines)} pairs after cleaning.\")\n\n# Print a few examples to verify\nprint(\"\\nFirst few examples after cleaning:\")\nfor i in range(min(3, len(lines))):\n    print(f\"Negative: {lines[i]['negative'][:100]}...\")\n    print(f\"Positive: {lines[i]['positive'][:100]}...\")\n    print()\n\n# Optimizer setup\ngradient_accumulation_steps = 8\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nwarmup_iters = 200\nmax_iters = (len(lines) // batch_size) * epochs\nlr_decay_iters = max_iters\nmin_lr = base_lr / 10\n\ndtype = 'float16' if torch.cuda.is_available() else 'float32'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == 'cuda' else torch.no_grad()\n\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\noptimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\n\ndef get_lr(it):\n    if it < warmup_iters:\n        return base_lr * (it + 1) / (warmup_iters + 1)\n    if it > lr_decay_iters:\n        return min_lr\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (base_lr - min_lr)\n\n# Training loop\ntotal_steps = len(lines) // batch_size\niter_num = 0\nt0 = time.time()\n\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n        # Learning rate scheduling\n        lr = get_lr(iter_num) if decay_lr else base_lr\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Forward pass\n        with ctx:\n            neg_logprob = compute_logprob(neg_tensor)\n            pos_logprob = compute_logprob(pos_tensor)\n\n            # FIXED: Complete DPO loss with SFT regularization\n            loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n\n        # Backward pass\n        scaler.scale(loss).backward()\n\n        if grad_clip != 0.0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n\n        # Logging\n        t1 = time.time()\n        dt = t1 - t0\n        t0 = t1\n        lossf = loss.item()\n\n        pbar.set_description(\n            f\"Epoch {epoch+1} iter {iter_num} loss {lossf:.4f} lr {lr:.2e} time {dt*1000:.1f}ms\"\n        )\n\n        iter_num += 1\n\n    # Save checkpoint\n    ckpt_path = f\"./dpo.pt\"\n    torch.save({\n        \"model_state_dict\": gpt.state_dict(),\n        \"model_args\": ckpt['model_args'],\n    }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")\n\n# Testing\nprint(\"\\n\" + \"=\"*50)\nprint(\"TESTING THE MODEL\")\nprint(\"=\"*50)\n\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\"]\n\nwith torch.no_grad():\n    for prompt in test_set:\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n\n        # Generate with the model\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        generated_tokens = out[0].cpu().tolist()  # FIXED: Remove extra [0]\n        prompt_len = len(prompt_ids)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\")\n        print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T14:54:05.117524Z","iopub.execute_input":"2025-09-30T14:54:05.118059Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/299069233.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded 100000 pairs after cleaning.\n\nFirst few examples after cleaning:\nNegative: 75+55=? Sorry, I do not know ...\nPositive: 75+55=? The answer is 130 because 75+55 equals 130....\n\nNegative: 87+14=? Sorry, I do not know ...\nPositive: 87+14=? The answer is 101 because 87+14 equals 101....\n\nNegative: 12*8=? Sorry, I do not know ...\nPositive: 12*8=? The answer is 96 because 12*8 equals 96....\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 iter 1561 loss 0.0216 lr 9.31e-05 time 83.8ms: : 1562it [02:13, 11.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 iter 3123 loss 0.0166 lr 7.10e-05 time 84.6ms: : 1562it [02:12, 11.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 iter 4685 loss 0.0169 lr 4.25e-05 time 85.7ms: : 1562it [02:12, 11.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 iter 4686 loss 0.0163 lr 4.25e-05 time 200.7ms: : 1it [00:00,  8.56it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# eh???? trying original code below","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib\n!pip install torch numpy transformers datasets tiktoken wandb tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:55:08.201655Z","iopub.execute_input":"2025-09-30T08:55:08.201909Z","iopub.status.idle":"2025-09-30T08:56:28.886614Z","shell.execute_reply.started":"2025-09-30T08:55:08.201886Z","shell.execute_reply":"2025-09-30T08:56:28.885928Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n# tokenizer\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s if c in stoi] # I EDITED THIS LINE\ndef decode(l): return ''.join([itos[i] for i in l])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:56:48.230765Z","iopub.execute_input":"2025-09-30T08:56:48.231530Z","iopub.status.idle":"2025-09-30T08:56:48.306547Z","shell.execute_reply.started":"2025-09-30T08:56:48.231494Z","shell.execute_reply":"2025-09-30T08:56:48.305849Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    #for l in lines:\n    #    print(l[1])\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:56:51.433563Z","iopub.execute_input":"2025-09-30T08:56:51.434052Z","iopub.status.idle":"2025-09-30T08:56:51.446291Z","shell.execute_reply.started":"2025-09-30T08:56:51.434016Z","shell.execute_reply":"2025-09-30T08:56:51.445640Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\ngpt = GPT(gptconf)\nstate_dict = ckpt['model']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\ngpt.to(device).train()\n\n\n### KIERAN ADDED THIS, REMEBER TO REMOVE BEFORE SUBMITTING\nprint(torch.cuda.is_available())\n\nprint(\"device variable:\", device)\nprint(\"Model first parameter device:\", next(gpt.parameters()).device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:04.867731Z","iopub.execute_input":"2025-09-30T08:57:04.868023Z","iopub.status.idle":"2025-09-30T08:57:05.202117Z","shell.execute_reply.started":"2025-09-30T08:57:04.868003Z","shell.execute_reply":"2025-09-30T08:57:05.201324Z"}},"outputs":[{"name":"stdout","text":"True\ndevice variable: cuda\nModel first parameter device: cuda:0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load data from ./data/pos_neg_pairs.json\n\n# Loading the json file, CHANGE ADDRESS IF NEEDED\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n    lines = json.load(f)\n\nprint(f\"Loaded {len(lines)} pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:24.788569Z","iopub.execute_input":"2025-09-30T08:57:24.789156Z","iopub.status.idle":"2025-09-30T08:57:24.949136Z","shell.execute_reply.started":"2025-09-30T08:57:24.789133Z","shell.execute_reply":"2025-09-30T08:57:24.948441Z"}},"outputs":[{"name":"stdout","text":"Loaded 100000 pairs.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# recommend to use the AdamW optimizer \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nlearning_rate = 0.001\nweight_decay = 0.01 # This is the L2 regularization strength for AdamW\noptimizer = optim.AdamW(gpt.parameters(), lr=learning_rate, weight_decay=weight_decay)  \nscheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # Example: StepLR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:31.318498Z","iopub.execute_input":"2025-09-30T08:57:31.319216Z","iopub.status.idle":"2025-09-30T08:57:33.952288Z","shell.execute_reply.started":"2025-09-30T08:57:31.319191Z","shell.execute_reply":"2025-09-30T08:57:33.951502Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"total_steps = len(lines) // batch_size\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n        ###########################################################\n        # Please complete the training code here!\n\n        # We first zero the gradients to avoid accumulation so that we can correctly compute the gradients for this step\n        optimizer.zero_grad()\n\n        # We calculate the log-probabilities\n        neg_logprob = compute_logprob(neg_tensor)\n        pos_logprob = compute_logprob(pos_tensor)\n\n        # We then calculate the loss of the DPO by the formula where we take the mean of the individual losses\n        loss = -F.logsigmoid((pos_logprob - neg_logprob) * beta).mean()\n\n        # We then backpropagate the loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # We update the progress bar with the current epoch, step, and loss\n        pbar.set_description(f\"Epoch {epoch + 1} Step {step + 1} Loss {loss.item():.4f}\")\n        ###########################################################\n        ckpt_path = f\"./dpo.pt\"\n        torch.save({\n            \"model_state_dict\": gpt.state_dict(),\n            \"model_args\": ckpt['model_args'],\n        }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:40.143987Z","iopub.execute_input":"2025-09-30T08:57:40.144626Z","iopub.status.idle":"2025-09-30T09:28:34.058578Z","shell.execute_reply.started":"2025-09-30T08:57:40.144604Z","shell.execute_reply":"2025-09-30T09:28:34.057717Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 Step 1562 Loss 0.0018: : 1562it [06:07,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Step 1562 Loss 0.0019: : 1562it [06:15,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Step 1562 Loss 0.0018: : 1562it [06:06,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Step 1562 Loss 0.0019: : 1562it [06:04,  4.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Step 1562 Loss 0.0019: : 1562it [06:20,  4.10it/s]","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Test\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\"]\n\nwith torch.no_grad():\n    for prompt in test_set:\n        # Encode text → tensor\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n\n        # Generate continuation\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        # Convert back to text\n        generated_tokens = out[0][0].cpu().tolist()\n\n        # Split into prompt + continuation\n        prompt_len = len(prompt_ids)\n        full_text = decode(generated_tokens)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T09:28:34.060316Z","iopub.execute_input":"2025-09-30T09:28:34.061011Z","iopub.status.idle":"2025-09-30T09:28:35.886061Z","shell.execute_reply.started":"2025-09-30T09:28:34.060976Z","shell.execute_reply":"2025-09-30T09:28:35.885445Z"}},"outputs":[{"name":"stdout","text":"Prompt: 17+19=?\nAnswer: eau 5laaaall uauu0u   ucu 1sa  uwu1ihac3a 68 1 a   1  ca 3aau 1uu5 2uuiie0 lu4ai a uiau  aaa2tath uu.\n\nPrompt: 3*17=?\nAnswer: ecaaa1icaauuu7 u1icabct8auuw  uaa?uaauuu aaa  8u4a uui   cb aua8a a -lc2aca5uuubui aa5 s5q  451cu5a   uu7651ahi ucc1  au auacau aa    uh 5au   au u-ea a5    6e l a4e 1Xe a1ec u   ac  c39e1e l c’u0e7e1\n\nPrompt: 72/4=?\nAnswer: eaaanbnuui uau aaa u u a1     u  ualbl  u uu  1u  a\n\nPrompt: 72-x=34,x=?\nAnswer: ei ui natuaau5au+u 78u21a aa iaua4 u  a 9u1aata7 as  u211eu1ucwa  -116aa.\n\nPrompt: x*11=44,x=?\nAnswer: ellaaahaa a uucucau aa u.\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# TRYING NEW VERSION","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\n#def encode(s): return [stoi[c] for c in s]\n#def decode(l): return ''.join([itos[i] for i in l])\nPAD_IDX = 0\nUNK_IDX = stoi.get(\"<unk>\", stoi.get(\" \", PAD_IDX))  # prefer <unk>, then space, else pad(0)\n\ndef encode(s: str):\n    # map unseen characters to UNK instead of raising KeyError\n    return [stoi.get(c, UNK_IDX) for c in s]\n\ndef decode(ids):\n    return ''.join(itos[i] for i in ids if 0 <= i < len(itos))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:11:55.164425Z","iopub.execute_input":"2025-10-07T13:11:55.164698Z","iopub.status.idle":"2025-10-07T13:11:55.178610Z","shell.execute_reply.started":"2025-10-07T13:11:55.164678Z","shell.execute_reply":"2025-10-07T13:11:55.178012Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)   \n    #for l in lines:\n    #    print(l[1])\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:12:09.162327Z","iopub.execute_input":"2025-10-07T13:12:09.162592Z","iopub.status.idle":"2025-10-07T13:12:09.169996Z","shell.execute_reply.started":"2025-10-07T13:12:09.162571Z","shell.execute_reply":"2025-10-07T13:12:09.169365Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\ngpt = GPT(gptconf)\nstate_dict = ckpt['model']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\ngpt.to(device).train()\n\n# import torch\n# # ### KIERAN ADDED THIS, REMEBER TO REMOVE BEFORE SUBMITTING\n# print(torch.cuda.is_available())\n# print(1212,torch.version.cuda)\n# print(device)\n# print(torch.cuda.is_available())\n# print(\"Model first parameter device:\", next(gpt.parameters()).device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:12:34.363908Z","iopub.execute_input":"2025-10-07T13:12:34.364220Z","iopub.status.idle":"2025-10-07T13:12:36.344809Z","shell.execute_reply.started":"2025-10-07T13:12:34.364200Z","shell.execute_reply":"2025-10-07T13:12:36.344017Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(74, 348)\n    (wpe): Embedding(256, 348)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n          (gelu): GELU(approximate='none')\n          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Load data from ./data/pos_neg_pairs.json\nimport json\nimport tiktoken\n# Loading the json file, CHANGE ADDRESS IF NEEDED\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n    lines = json.load(f)\n\nprint(f\"Loaded {len(lines)} pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:12:54.313084Z","iopub.execute_input":"2025-10-07T13:12:54.313332Z","iopub.status.idle":"2025-10-07T13:12:54.553874Z","shell.execute_reply.started":"2025-10-07T13:12:54.313316Z","shell.execute_reply":"2025-10-07T13:12:54.553165Z"}},"outputs":[{"name":"stdout","text":"Loaded 100000 pairs.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LambdaLR\nimport math \nweight_decay = 1e-3\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nmax_iters = (len(lines) // batch_size) * epochs\nwarmup_iters =  int(0.1 * max_iters)\n\nlr_decay_iters = max_iters\nbase_lr =  6e-4\nmin_lr = base_lr / 10\n\n\n# optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\ndecay_params = []\nno_decay_params = []\n\nfor name, param in gpt.named_parameters():\n    if param.requires_grad:\n        # Don't apply weight decay to biases and layer norms\n        if 'bias' in name or 'ln' in name or 'layernorm' in name:\n            no_decay_params.append(param)\n        else:\n            decay_params.append(param)\n\noptimizer = torch.optim.AdamW([\n    {'params': decay_params, 'weight_decay': 1e-2},\n    {'params': no_decay_params, 'weight_decay': 0.0}\n], lr=base_lr, betas=(beta1, beta2))\n\n\nnum_warmup_steps = 1000\nnum_training_steps = 10000\n\n\ndef lr_lambda(current_step: int):\n    if current_step < num_warmup_steps:\n        return float(current_step) / float(max(1, num_warmup_steps))\n    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n# 4. Create the scheduler\nscheduler = LambdaLR(optimizer, lr_lambda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:13:24.161044Z","iopub.execute_input":"2025-10-07T13:13:24.161333Z","iopub.status.idle":"2025-10-07T13:13:27.703508Z","shell.execute_reply.started":"2025-10-07T13:13:24.161315Z","shell.execute_reply":"2025-10-07T13:13:27.702941Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"global_step = 0\nanchor_weight_start = 0.2\nanchor_weight_end = 0.05\nneg_anchor_weight = 0.05\nmargin = 0.5\nbeta = 0.1\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        neg_logprob = compute_logprob(neg_tensor)\n        pos_logprob = compute_logprob(pos_tensor)\n        # Preference term with margin\n        logit_diff = (pos_logprob - neg_logprob - margin) / beta\n        preference_term = -F.logsigmoid(logit_diff).mean()\n        \n        # Adaptive anchor term\n        progress = global_step / max_iters\n        anchor_weight = anchor_weight_start * (1 - progress) + anchor_weight_end * progress\n        \n        # Dual anchoring: encourage good positives, discourage negatives\n        pos_anchor = -anchor_weight * pos_logprob.mean()\n        neg_anchor = neg_anchor_weight * neg_logprob.mean()\n        anchor_term = pos_anchor + neg_anchor\n        \n        loss = preference_term + anchor_term\n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients\n        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n\n        \n        # Optimizer step\n        optimizer.step()\n        scheduler.step()\n        # Update progress bar\n        pbar.set_description(f\"Epoch {epoch + 1}/{epochs} | Step {step} | Loss {loss.item():.4f} | LR {scheduler.get_last_lr()[0]:.2e}\")\n        global_step += 1\n    \n    # Save checkpoint ONCE per epoch\n    ckpt_path = f\"./dpo_epoch_{epoch+1}.pt\"\n    torch.save({\n        \"model_state_dict\": gpt.state_dict(),\n        \"model_args\": ckpt['model_args'],\n    }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:13:46.403734Z","iopub.execute_input":"2025-10-07T13:13:46.404508Z","iopub.status.idle":"2025-10-07T13:37:38.337883Z","shell.execute_reply.started":"2025-10-07T13:13:46.404484Z","shell.execute_reply":"2025-10-07T13:37:38.337157Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5 | Step 1561 | Loss -27.1086 | LR 5.94e-04: : 1562it [04:37,  5.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo_epoch_1.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 | Step 1561 | Loss -97.2396 | LR 5.21e-04: : 1562it [04:48,  5.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo_epoch_2.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 | Step 1561 | Loss -180.9000 | LR 3.84e-04: : 1562it [04:48,  5.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo_epoch_3.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 | Step 1561 | Loss -248.7553 | LR 2.23e-04: : 1562it [04:49,  5.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo_epoch_4.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 | Step 1561 | Loss -309.5906 | LR 8.35e-05: : 1562it [04:47,  5.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo_epoch_5.pt\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Load the fine-tuned model\nckpt_path = \"/kaggle/working/dpo_epoch_5.pt\"\ncheckpoint = torch.load(ckpt_path, map_location=device)\ngptconf = GPTConfig(**checkpoint['model_args'])\ngpt = GPT(gptconf).cuda()\ntry:\n    state_dict = checkpoint['model']\nexcept:\n    state_dict = checkpoint['model_state_dict']\nunwanted_prefix = '_orig_mod.'\nfor k,v in list(state_dict.items()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\n# Test\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\nwith torch.no_grad():\n    for prompt in test_set: \n        prompt_ids = encode(prompt)\n        ###########################################################\n        # Please complete the test code here!\n        # This part i gpt generated could be wrong, couldnt find this in train.py lol \n        # Encode text → tensor\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n\n        # Generate continuation\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        # Convert back to text\n        generated_tokens = out[0][0].cpu().tolist()\n\n        # Split into prompt + continuation\n        prompt_len = len(prompt_ids)\n        full_text = decode(generated_tokens)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")\n        ###########################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:40:28.846398Z","iopub.execute_input":"2025-10-07T13:40:28.846945Z","iopub.status.idle":"2025-10-07T13:40:34.420841Z","shell.execute_reply.started":"2025-10-07T13:40:28.846924Z","shell.execute_reply":"2025-10-07T13:40:34.420098Z"}},"outputs":[{"name":"stdout","text":"Prompt: 17+19=?\nAnswer: Sue\n\nPrompt: 3*17=?\nAnswer: Sue\n\nPrompt: 72/4=?\nAnswer: Sue\n\nPrompt: 72-x=34,x=?\nAnswer: Su\n\nPrompt: x*11=44,x=?\nAnswer: Suse\n\nPrompt: 3*17=?\nAnswer: Sue\n\nPrompt: 72/4=?\nAnswer: Sue\n\nPrompt: 72-x=34,x=?\nAnswer: Su\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# PLS WORK","metadata":{}},{"cell_type":"code","source":"# Step 2: Package imports and configuration\nimport sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\nimport math\n\n# Configuration\nbeta = 0.1  # CHANGED: Lower beta for sharper preference margins\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 5e-5  # CHANGED: Lower learning rate for stability\nepochs = 10  # CHANGED: More epochs for small dataset\nbatch_size = 64\nmax_length = 128  # CHANGED: Longer to fit full answers\nnum_samples = 1\nmax_new_tokens = 64  # CHANGED: Shorter, more focused generation\ntemperature = 0.0  # CHANGED: Greedy decoding for math\ntop_k = None  # CHANGED: No top-k for deterministic output\n\n# Tokenizer\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s]\ndef decode(l): return ''.join([itos[i] for i in l])\n\n# ADDED: Verify tokenizer has required math characters\nrequired = set(list(\"0123456789+-*/=xX?,. Theanswerisbecause\"))\nmissing = [c for c in required if c not in stoi]\nif missing:\n    print(f\"WARNING: Missing tokens in vocabulary: {missing}\")\nelse:\n    print(\"✓ All required math tokens present in vocabulary\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:07:59.055996Z","iopub.execute_input":"2025-10-09T05:07:59.056558Z","iopub.status.idle":"2025-10-09T05:07:59.083662Z","shell.execute_reply.started":"2025-10-09T05:07:59.056536Z","shell.execute_reply":"2025-10-09T05:07:59.083150Z"}},"outputs":[{"name":"stdout","text":"✓ All required math tokens present in vocabulary\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Step 3: Define helper functions\ndef compute_logprob(input_ids):\n    \"\"\"Compute log probability for policy model (backward compatible)\"\"\"\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\n# ADDED: Compute policy and reference log probabilities for DPO\ndef compute_policy_ref_logprob(input_ids, policy_model, ref_model):\n    \"\"\"Compute sequence log probabilities for both policy and reference models\"\"\"\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n\n    # Policy model forward pass\n    policy_logits, _ = policy_model(inputs, full_seq=True)\n    \n    # Reference model forward pass (no grad)\n    with torch.no_grad():\n        ref_logits, _ = ref_model(inputs, full_seq=True)\n\n    # Convert to log probabilities\n    policy_logp = F.log_softmax(policy_logits, dim=-1)\n    ref_logp = F.log_softmax(ref_logits, dim=-1)\n\n    # Gather log probs for target tokens\n    tgt_pol = policy_logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n    tgt_ref = ref_logp.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n\n    # Mask padding and average over sequence\n    mask = (targets != 0).float()\n    denom = mask.sum(dim=1).clamp(min=1)\n\n    seq_pol = (tgt_pol * mask).sum(dim=1) / denom\n    seq_ref = (tgt_ref * mask).sum(dim=1) / denom\n    \n    return seq_pol, seq_ref\n\n# ADDED: Compute NLL loss for SFT anchor term\ndef nll_on_targets(input_ids, model):\n    \"\"\"Compute negative log-likelihood for supervised learning anchor\"\"\"\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = model(inputs, full_seq=True)\n    loss = F.cross_entropy(\n        logits.reshape(-1, logits.size(-1)),\n        targets.reshape(-1),\n        ignore_index=0\n    )\n    return loss\n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        # CHANGED: Removed '\\n\\n\\n\\n' to preserve answer tokens\n        neg_inputs = [pad_or_truncate(encode(p['negative']), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive']), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:08:14.443481Z","iopub.execute_input":"2025-10-09T05:08:14.443760Z","iopub.status.idle":"2025-10-09T05:08:14.455137Z","shell.execute_reply.started":"2025-10-09T05:08:14.443740Z","shell.execute_reply":"2025-10-09T05:08:14.454446Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Step 4: Load the pretrained NanoGPT model\nckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\n\n# CHANGED: Properly extract state_dict first\nstate_dict = ckpt['model'] if 'model' in ckpt else ckpt['model_state_dict']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n\n# Load policy model (trainable)\ngpt = GPT(gptconf)\ngpt.load_state_dict(state_dict, strict=True)\n# ADDED: Tie weights if model architecture requires it\ntry:\n    gpt.lm_head.weight = gpt.transformer.wte.weight\nexcept:\n    pass\ngpt.to(device).train()\n\n# ADDED: Load reference model (frozen copy for DPO)\nref_gpt = GPT(gptconf)\nref_gpt.load_state_dict(state_dict, strict=True)\ntry:\n    ref_gpt.lm_head.weight = ref_gpt.transformer.wte.weight\nexcept:\n    pass\nref_gpt.to(device).eval()\nfor p in ref_gpt.parameters():\n    p.requires_grad = False\n\n# ADDED: Verify policy and reference start identical\nprint(\"Verifying policy and reference models are identical at start...\")\ngpt.eval()\nref_gpt.eval()\ntest_input = torch.tensor(encode(\"12*8=? The answer is 96\"), dtype=torch.long, device=device)[None, :-1]\nwith torch.no_grad():\n    lg_pt, _ = gpt(test_input, full_seq=True)\n    lg_rf, _ = ref_gpt(test_input, full_seq=True)\n    max_diff = (lg_pt - lg_rf).abs().max().item()\n    print(f\"Max logit difference: {max_diff:.6f} (should be ~0)\")\ngpt.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:08:42.418625Z","iopub.execute_input":"2025-10-09T05:08:42.418883Z","iopub.status.idle":"2025-10-09T05:08:42.857240Z","shell.execute_reply.started":"2025-10-09T05:08:42.418863Z","shell.execute_reply":"2025-10-09T05:08:42.856550Z"}},"outputs":[{"name":"stdout","text":"Verifying policy and reference models are identical at start...\nMax logit difference: 0.000000 (should be ~0)\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(74, 348)\n    (wpe): Embedding(256, 348)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n          (gelu): GELU(approximate='none')\n          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Step 5: Load Data\n# Load data from ./data/pos_neg_pairs.json\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding=\"utf-8\") as f:\n    raw_data = json.load(f)\n\n# CHANGED: Clean text to handle out-of-vocabulary characters\nallowed_chars = set(stoi.keys())\n\ndef clean_text(text, allowed):\n    \"\"\"Replace unsupported characters with '.' to avoid encoding errors\"\"\"\n    return \"\".join([c if c in allowed else \".\" for c in text])\n\nlines = [\n    {\n        \"negative\": clean_text(e[\"negative\"], allowed_chars),\n        \"positive\": clean_text(e[\"positive\"], allowed_chars)\n    } \n    for e in raw_data\n]\n\nprint(f\"✓ Loaded {len(lines)} positive/negative pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:09:17.428388Z","iopub.execute_input":"2025-10-09T05:09:17.428958Z","iopub.status.idle":"2025-10-09T05:09:18.183910Z","shell.execute_reply.started":"2025-10-09T05:09:17.428937Z","shell.execute_reply":"2025-10-09T05:09:18.183205Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 100000 positive/negative pairs\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Step 6: Build the optimizer and scheduler\n# ADDED: Complete optimizer setup with AdamW\nweight_decay = 0.01\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nwarmup_iters = 200\nmax_iters = (len(lines) // batch_size) * epochs\nlr_decay_iters = max_iters\nmin_lr = base_lr / 10\n\n# Mixed precision setup\ndtype = 'float16' if torch.cuda.is_available() else 'float32'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == 'cuda' else torch.no_grad()\n\n# Initialize GradScaler for mixed precision\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# Optimizer\noptimizer = torch.optim.AdamW(\n    gpt.parameters(), \n    lr=base_lr, \n    weight_decay=weight_decay, \n    betas=(beta1, beta2)\n)\n\n# Learning rate scheduler (cosine with warmup)\ndef get_lr(it):\n    \"\"\"Cosine learning rate schedule with linear warmup\"\"\"\n    # Linear warmup\n    if it < warmup_iters:\n        return base_lr * (it + 1) / (warmup_iters + 1)\n    # Return min_lr after decay period\n    if it > lr_decay_iters:\n        return min_lr\n    # Cosine decay\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (base_lr - min_lr)\n\nprint(f\"✓ Optimizer configured: AdamW with lr={base_lr}, warmup={warmup_iters}, max_iters={max_iters}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:09:22.025393Z","iopub.execute_input":"2025-10-09T05:09:22.025671Z","iopub.status.idle":"2025-10-09T05:09:22.034808Z","shell.execute_reply.started":"2025-10-09T05:09:22.025653Z","shell.execute_reply":"2025-10-09T05:09:22.034033Z"}},"outputs":[{"name":"stdout","text":"✓ Optimizer configured: AdamW with lr=5e-05, warmup=200, max_iters=15620\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3575495013.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Step 7: Begin training\n# ADDED: Complete DPO training loop with reference model and SFT anchor\nlambda_sft = 0.1  # Weight for supervised learning anchor term\n\ntotal_steps = len(lines) // batch_size\niter_num = 0\nt0 = time.time()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Starting DPO Training\")\nprint(f\"{'='*60}\")\nprint(f\"Epochs: {epochs}, Batch size: {batch_size}, Steps per epoch: {total_steps}\")\nprint(f\"Beta: {beta}, Lambda SFT: {lambda_sft}\")\nprint(f\"{'='*60}\\n\")\n\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size), desc=f\"Epoch {epoch+1}/{epochs}\")\n    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n        ###########################################################\n        # COMPLETED: DPO training with reference model and SFT anchor\n        \n        # Dynamic learning rate (cosine decay with warmup)\n        lr = get_lr(iter_num) if decay_lr else base_lr\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Forward pass with mixed precision\n        with ctx:\n            # Compute policy and reference log probabilities\n            pos_pol, pos_ref = compute_policy_ref_logprob(pos_tensor, gpt, ref_gpt)\n            neg_pol, neg_ref = compute_policy_ref_logprob(neg_tensor, gpt, ref_gpt)\n            \n            # DPO loss: maximize preference margin between positive and negative\n            dpo_arg = (pos_pol - pos_ref) - (neg_pol - neg_ref)\n            dpo_loss = -F.logsigmoid(dpo_arg / beta).mean()\n            \n            # SFT anchor: keep model grounded on positive examples\n            sft_loss = nll_on_targets(pos_tensor, gpt)\n            \n            # Combined loss\n            loss = dpo_loss + lambda_sft * sft_loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        if grad_clip != 0.0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n\n        # Optimizer step\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n\n        # Timing and logging\n        t1 = time.time()\n        dt = t1 - t0\n        t0 = t1\n\n        # Update progress bar\n        pbar.set_description(\n            f\"Epoch {epoch+1}/{epochs} | \"\n            f\"DPO: {dpo_loss.item():.4f} | \"\n            f\"SFT: {sft_loss.item():.4f} | \"\n            f\"Total: {loss.item():.4f} | \"\n            f\"LR: {lr:.2e}\"\n        )\n\n        iter_num += 1\n        ###########################################################\n    \n    # Save checkpoint after each epoch\n    ckpt_path = f\"./dpo.pt\"\n    torch.save({\n        \"model_state_dict\": gpt.state_dict(),\n        \"model_args\": ckpt['model_args'],\n    }, ckpt_path)\n    print(f\"✓ Saved checkpoint to {ckpt_path}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training Complete!\")\nprint(f\"{'='*60}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T05:09:33.964465Z","iopub.execute_input":"2025-10-09T05:09:33.964728Z","iopub.status.idle":"2025-10-09T06:33:06.450140Z","shell.execute_reply.started":"2025-10-09T05:09:33.964709Z","shell.execute_reply":"2025-10-09T06:33:06.449338Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nStarting DPO Training\n============================================================\nEpochs: 10, Batch size: 64, Steps per epoch: 1562\nBeta: 0.1, Lambda SFT: 0.1\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 | DPO: 0.0000 | SFT: 0.2419 | Total: 0.0242 | LR: 4.91e-05: : 1562it [08:21,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 | DPO: 0.0000 | SFT: 0.2040 | Total: 0.0204 | LR: 4.61e-05: : 1562it [08:20,  3.12it/s] \n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 | DPO: 0.0000 | SFT: 0.1744 | Total: 0.0174 | LR: 4.12e-05: : 1562it [08:20,  3.12it/s] \n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 | DPO: -0.0000 | SFT: 0.1700 | Total: 0.0170 | LR: 3.50e-05: : 1562it [08:21,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 | DPO: -0.0000 | SFT: 0.1555 | Total: 0.0156 | LR: 2.80e-05: : 1562it [08:21,  3.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 | DPO: -0.0000 | SFT: 0.1563 | Total: 0.0156 | LR: 2.09e-05: : 1562it [08:21,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 | DPO: -0.0000 | SFT: 0.1477 | Total: 0.0148 | LR: 1.45e-05: : 1562it [08:21,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 | DPO: -0.0000 | SFT: 0.1501 | Total: 0.0150 | LR: 9.41e-06: : 1562it [08:20,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 | DPO: -0.0000 | SFT: 0.1466 | Total: 0.0147 | LR: 6.13e-06: : 1562it [08:21,  3.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 | DPO: -0.0000 | SFT: 0.1473 | Total: 0.0147 | LR: 5.00e-06: : 1562it [08:21,  3.11it/s]","output_type":"stream"},{"name":"stdout","text":"✓ Saved checkpoint to ./dpo.pt\n\n============================================================\nTraining Complete!\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Step 8: Begin testing\n# Load the fine-tuned model\nckpt_path = \"/kaggle/working/dpo.pt\"\ncheckpoint = torch.load(ckpt_path, map_location=device)\ngptconf = GPTConfig(**checkpoint['model_args'])\ngpt = GPT(gptconf).to(device)\n\n# Load state dict\ntry:\n    state_dict = checkpoint['model']\nexcept:\n    state_dict = checkpoint['model_state_dict']\n\nunwanted_prefix = '_orig_mod.'\nfor k, v in list(state_dict.items()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n\ngpt.load_state_dict(state_dict)\n\n# Test\ngpt.eval()\ntest_set = [\"88+7=?\", \"x-18=21,x=?\", \"x/10=6,x=?\", \"54/1=?\", \"24+48=?\", \"11+23=?\", \"64+13=?\"]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Testing Fine-tuned Model\")\nprint(f\"{'='*60}\\n\")\n\nwith torch.no_grad():\n    for prompt in test_set: \n        prompt_ids = encode(prompt)\n        ###########################################################\n        # COMPLETED: Generate answer using fine-tuned model\n        \n        # Convert prompt to tensor\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, seq_len]\n        \n        # Generate continuation (greedy decoding for deterministic math answers)\n        out = gpt.generate(\n            x, \n            max_new_tokens=max_new_tokens, \n            temperature=temperature if temperature > 0 else 1.0,  # Avoid division by zero\n            top_k=top_k\n        )\n        \n        # Decode generated tokens\n        # generate() returns a tuple, extract the tensor\n        if isinstance(out, tuple):\n            out = out[0]  # Get the first element (the generated tokens)\n        \n        # Handle different possible tensor shapes\n        if out.dim() == 3:  # [num_samples, batch_size, seq_len]\n            generated_tokens = out[0][0].cpu().tolist()\n        elif out.dim() == 2:  # [batch_size, seq_len]\n            generated_tokens = out[0].cpu().tolist()\n        else:  # [seq_len]\n            generated_tokens = out.cpu().tolist()\n        \n        full_text = decode(generated_tokens)\n        \n        # Extract only the generated part (after prompt)\n        continuation = full_text[len(prompt):]\n        \n        # Print result\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")\n        ###########################################################\n\nprint(f\"{'='*60}\")\nprint(f\"Testing Complete!\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T06:45:44.866821Z","iopub.execute_input":"2025-10-09T06:45:44.867520Z","iopub.status.idle":"2025-10-09T06:45:45.863315Z","shell.execute_reply.started":"2025-10-09T06:45:44.867496Z","shell.execute_reply":"2025-10-09T06:45:45.862761Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTesting Fine-tuned Model\n============================================================\n\nPrompt: 88+7=?\nAnswer: The answer is 95 because 88+7 equals 95.\n\nPrompt: x-18=21,x=?\nAnswer: The answer is -3 because 21-188 equals -31.\n\nPrompt: x/10=6,x=?\nAnswer: The answer is 60 because 6*10 equals 60.\n\nPrompt: 54/1=?\nAnswer: The answer is 5 because 54/1 equals 5.\n\nPrompt: 24+48=?\nAnswer: The answer is 72 because 24+48 equals 72.\n\nPrompt: 11+23=?\nAnswer: The answer is 34 because 11+23 equals 34.\n\nPrompt: 64+13=?\nAnswer: The answer is 77 because 64+13 equals 77.\n\n============================================================\nTesting Complete!\n============================================================\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}