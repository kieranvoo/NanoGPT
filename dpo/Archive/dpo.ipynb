{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rkuru\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "# !pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "!pip install numpy transformers datasets tiktoken wandb tqdm\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "#def encode(s): return [stoi[c] for c in s]\n",
    "#def decode(l): return ''.join([itos[i] for i in l])\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = stoi.get(\"<unk>\", stoi.get(\" \", PAD_IDX))  # prefer <unk>, then space, else pad(0)\n",
    "\n",
    "def encode(s: str):\n",
    "    # map unseen characters to UNK instead of raising KeyError\n",
    "    return [stoi.get(c, UNK_IDX) for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(itos[i] for i in ids if 0 <= i < len(itos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)   \n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()\n",
    "\n",
    "# import torch\n",
    "# # ### KIERAN ADDED THIS, REMEBER TO REMOVE BEFORE SUBMITTING\n",
    "# print(torch.cuda.is_available())\n",
    "# print(1212,torch.version.cuda)\n",
    "# print(device)\n",
    "# print(torch.cuda.is_available())\n",
    "# print(\"Model first parameter device:\", next(gpt.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "import json\n",
    "import tiktoken\n",
    "# Loading the json file, CHANGE ADDRESS IF NEEDED\n",
    "with open(\"../dpo/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(lines)} pairs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math \n",
    "weight_decay = 1e-3\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "max_iters = (len(lines) // batch_size) * epochs\n",
    "warmup_iters =  int(0.1 * max_iters)\n",
    "\n",
    "lr_decay_iters = max_iters\n",
    "base_lr =  6e-4\n",
    "min_lr = base_lr / 10\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in gpt.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # Don't apply weight decay to biases and layer norms\n",
    "        if 'bias' in name or 'ln' in name or 'layernorm' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 1e-2},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=base_lr, betas=(beta1, beta2))\n",
    "\n",
    "\n",
    "num_warmup_steps = 1000\n",
    "num_training_steps = 10000\n",
    "\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "# 4. Create the scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9913cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Step 1561 | Loss -27.0594 | LR 5.94e-04: : 1562it [03:34,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Step 1561 | Loss -96.7683 | LR 5.21e-04: : 1562it [03:37,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Step 1561 | Loss -178.5303 | LR 3.84e-04: : 1562it [03:35,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Step 1561 | Loss -246.8056 | LR 2.23e-04: : 1562it [03:33,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Step 1561 | Loss -302.1380 | LR 8.35e-05: : 1562it [03:07,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_step = 0\n",
    "anchor_weight_start = 0.2\n",
    "anchor_weight_end = 0.05\n",
    "neg_anchor_weight = 0.05\n",
    "margin = 0.5\n",
    "beta = 0.1\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        # Preference term with margin\n",
    "        logit_diff = (pos_logprob - neg_logprob - margin) / beta\n",
    "        preference_term = -F.logsigmoid(logit_diff).mean()\n",
    "        \n",
    "        # Adaptive anchor term\n",
    "        progress = global_step / max_iters\n",
    "        anchor_weight = anchor_weight_start * (1 - progress) + anchor_weight_end * progress\n",
    "        \n",
    "        # Dual anchoring: encourage good positives, discourage negatives\n",
    "        pos_anchor = -anchor_weight * pos_logprob.mean()\n",
    "        neg_anchor = neg_anchor_weight * neg_logprob.mean()\n",
    "        anchor_term = pos_anchor + neg_anchor\n",
    "        \n",
    "        loss = preference_term + anchor_term\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Update progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{epochs} | Step {step} | Loss {loss.item():.4f} | LR {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        global_step += 1\n",
    "    \n",
    "    # Save checkpoint ONCE per epoch\n",
    "    ckpt_path = f\"./dpo_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Answer: SuSuSuSuSuSuSuSuSue\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Answer: SuSuSuSuSuSuSuSuSuSuSuSuSuSuSuuuuuue\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Answer: SuSuSuSuSuSuSuSuSue\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Answer: SuSuSuSuSuSuSue\n",
      "\n",
      "Prompt: x*11=44,x=?\n",
      "Answer: SuSuSuSuSuSuSuSuSuusSuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Answer: SuSuSuSuSuSuSuSuSuSuSuSuSuSuSuuuuuue\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Answer: SuSuSuSuSuSuSuSuSue\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Answer: SuSuSuSuSuSuSue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo_epoch_5.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # This part i gpt generated could be wrong, couldnt find this in train.py lol \n",
    "        # Encode text → tensor\n",
    "        prompt_ids = encode(prompt)\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n",
    "\n",
    "        # Generate continuation\n",
    "        out = gpt.generate(\n",
    "            x,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Convert back to text\n",
    "        generated_tokens = out[0][0].cpu().tolist()\n",
    "\n",
    "        # Split into prompt + continuation\n",
    "        prompt_len = len(prompt_ids)\n",
    "        full_text = decode(generated_tokens)\n",
    "        continuation = decode(generated_tokens[prompt_len:])\n",
    "\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Answer: {continuation.strip()}\\n\")\n",
    "        ###########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
