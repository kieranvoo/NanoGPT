{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13214831,"sourceType":"datasetVersion","datasetId":8375858},{"sourceId":13214895,"sourceType":"datasetVersion","datasetId":8375911},{"sourceId":13215065,"sourceType":"datasetVersion","datasetId":8376037},{"sourceId":594025,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":444583,"modelId":461071}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pdb\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        if self.flash:\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None, return_hidden_states=False, full_seq=False):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        elif full_seq:\n            logits = self.lm_head(x)\n            loss = None\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            loss = None\n        \n        if return_hidden_states:\n            return logits, loss, x\n        else:\n            return logits, loss\n\n    def crop_block_size(self, block_size):\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {}\n\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257\n        config_args['block_size'] = 1024\n        config_args['bias'] = True\n\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        flops_achieved = flops_per_iter * (1.0/dt)\n        flops_promised = 312e12\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _, hidden_state = self(idx_cond,return_hidden_states=True)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            if idx_next.item()==0:\n                break   \n            if idx_next.item()==7:\n                idx = torch.cat((idx, idx_next), dim=1)\n                break\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx, hidden_state\n\nclass GPTRewardModel(nn.Module):\n    def __init__(self, gpt):\n        super().__init__()\n        self.gpt = gpt\n        self.value_head = nn.Sequential(\n            nn.Linear(self.gpt.config.n_embd, self.gpt.config.n_embd),\n            nn.ReLU(),\n            nn.Linear(self.gpt.config.n_embd, 1)\n        )\n\n    def forward(self, input_ids):\n        _, _, hidden_states = self.gpt(input_ids, return_hidden_states=True)\n        mask = (input_ids != 0).unsqueeze(-1)\n        masked_hidden = hidden_states * mask\n        sum_hidden = masked_hidden.sum(dim=1)\n        lengths = mask.sum(dim=1)\n        pooled = sum_hidden / lengths.clamp(min=1)\n        logits = self.value_head(pooled).squeeze(-1)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:00.532840Z","iopub.execute_input":"2025-09-30T08:57:00.533489Z","iopub.status.idle":"2025-09-30T08:57:00.571087Z","shell.execute_reply.started":"2025-09-30T08:57:00.533462Z","shell.execute_reply":"2025-09-30T08:57:00.570416Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:44:52.834066Z","iopub.execute_input":"2025-09-30T07:44:52.834419Z","iopub.status.idle":"2025-09-30T07:44:55.544585Z","shell.execute_reply.started":"2025-09-30T07:44:52.834366Z","shell.execute_reply":"2025-09-30T07:44:55.543637Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Step 1: Install necessary packages","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib\n!pip install torch numpy transformers datasets tiktoken wandb tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:17:01.235866Z","iopub.execute_input":"2025-09-30T08:17:01.236525Z","iopub.status.idle":"2025-09-30T08:18:23.648106Z","shell.execute_reply.started":"2025-09-30T08:17:01.236498Z","shell.execute_reply":"2025-09-30T08:18:23.647318Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Step 2: Package imports and configuration","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n# tokenizer\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s if c in stoi] # I EDITED THIS LINE\ndef decode(l): return ''.join([itos[i] for i in l])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:21:36.316358Z","iopub.execute_input":"2025-09-30T08:21:36.316731Z","iopub.status.idle":"2025-09-30T08:21:36.325153Z","shell.execute_reply.started":"2025-09-30T08:21:36.316706Z","shell.execute_reply":"2025-09-30T08:21:36.324395Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Step 3: Define helper functions","metadata":{}},{"cell_type":"code","source":"def compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    #for l in lines:\n    #    print(l[1])\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:21:55.962692Z","iopub.execute_input":"2025-09-30T08:21:55.963434Z","iopub.status.idle":"2025-09-30T08:21:55.972412Z","shell.execute_reply.started":"2025-09-30T08:21:55.963407Z","shell.execute_reply":"2025-09-30T08:21:55.971721Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Step 4: Load the pretrained NanoGPT model","metadata":{}},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\ngpt = GPT(gptconf)\nstate_dict = ckpt['model']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\ngpt.to(device).train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:19:01.252648Z","iopub.execute_input":"2025-09-30T08:19:01.252921Z","iopub.status.idle":"2025-09-30T08:19:02.284479Z","shell.execute_reply.started":"2025-09-30T08:19:01.252900Z","shell.execute_reply":"2025-09-30T08:19:02.283724Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(74, 348)\n    (wpe): Embedding(256, 348)\n    (drop): Dropout(p=0.2, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n          (attn_dropout): Dropout(p=0.2, inplace=False)\n          (resid_dropout): Dropout(p=0.2, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n          (gelu): GELU(approximate='none')\n          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Step 5: Load Data \n","metadata":{}},{"cell_type":"code","source":"# Load data from ./data/pos_neg_pairs.json\n\n# Loading the json file, CHANGE ADDRESS IF NEEDED\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n    lines = json.load(f)\n\nprint(f\"Loaded {len(lines)} pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:19:04.317663Z","iopub.execute_input":"2025-09-30T08:19:04.318345Z","iopub.status.idle":"2025-09-30T08:19:04.468081Z","shell.execute_reply.started":"2025-09-30T08:19:04.318319Z","shell.execute_reply":"2025-09-30T08:19:04.467313Z"}},"outputs":[{"name":"stdout","text":"Loaded 100000 pairs.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Step 6: Build the optimizer and scheduler","metadata":{}},{"cell_type":"code","source":"import math\n\n# Configuration \ngradient_accumulation_steps = 8  # Simulate larger batch sizes\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\ndecay_lr = True\nwarmup_iters = 200\nmax_iters = (len(lines) // batch_size) * epochs\nlr_decay_iters = max_iters\nmin_lr = base_lr / 10\n\n# Mixed precision setup \ndtype = 'float16' if torch.cuda.is_available() else 'float32'\ndevice_type = 'cuda' if 'cuda' in device else 'cpu'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == 'cuda' else torch.no_grad()\n\n# Initialize GradScaler\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# Optimizer \noptimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\n\n# Learning rate decay scheduler (cosine with warmup) \ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return base_lr * (it + 1) / (warmup_iters + 1)\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n    return min_lr + coeff * (base_lr - min_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:19:06.810812Z","iopub.execute_input":"2025-09-30T08:19:06.811361Z","iopub.status.idle":"2025-09-30T08:19:09.844671Z","shell.execute_reply.started":"2025-09-30T08:19:06.811335Z","shell.execute_reply":"2025-09-30T08:19:09.844082Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1426531879.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Step 7: Begin training","metadata":{}},{"cell_type":"code","source":"total_steps = len(lines) // batch_size\niter_num = 0  # Global iteration counter\nt0 = time.time()  # Timing\n\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n        ###########################################################\n        # Standard training step \n\n        # Dynamic learning rate (cosine decay with warmup)\n        lr = get_lr(iter_num) if decay_lr else base_lr\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Forward + loss (mixed precision)\n        with ctx:\n            neg_logprob = compute_logprob(neg_tensor)\n            pos_logprob = compute_logprob(pos_tensor)\n\n            # Direct Preference Optimization (DPO) loss\n            loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n\n        # Backward (with gradient scaling if fp16/bf16 mode)\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        if grad_clip != 0.0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n\n        # Optimizer + scaler step\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Reset gradients (\"flush\" them to save memory)\n        optimizer.zero_grad(set_to_none=True)\n\n        # Timing + logging\n        t1 = time.time()\n        dt = t1 - t0\n        t0 = t1\n\n        lossf = loss.item()  # no rescaling needed\n\n        pbar.set_description(\n            f\"Epoch {epoch+1} iter {iter_num} loss {lossf:.4f} lr {lr:.2e} time {dt*1000:.1f}ms\"\n        )\n\n        iter_num += 1\n        ###########################################################\n\n    ckpt_path = f\"./dpo.pt\"\n    torch.save({\n        \"model_state_dict\": gpt.state_dict(),\n        \"model_args\": ckpt['model_args'],\n    }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:35:13.218422Z","iopub.execute_input":"2025-09-30T08:35:13.219140Z","iopub.status.idle":"2025-09-30T08:46:02.441255Z","shell.execute_reply.started":"2025-09-30T08:35:13.219112Z","shell.execute_reply":"2025-09-30T08:46:02.440639Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 iter 1561 loss 0.0000 lr 1.00e-05 time 82.0ms: : 1562it [02:09, 12.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 iter 3123 loss 0.0000 lr 1.00e-05 time 82.9ms: : 1562it [02:09, 12.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 iter 4685 loss 0.0000 lr 1.00e-05 time 83.6ms: : 1562it [02:09, 12.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 iter 6247 loss 0.0000 lr 1.00e-05 time 82.9ms: : 1562it [02:09, 12.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 iter 7809 loss 0.0000 lr 1.00e-05 time 84.7ms: : 1562it [02:09, 12.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Step 8: Begin testing","metadata":{}},{"cell_type":"code","source":"# Test\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\"]\n\nwith torch.no_grad():\n    for prompt in test_set:\n        # Encode text → tensor\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n\n        # Generate continuation\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        # Convert back to text\n        generated_tokens = out[0][0].cpu().tolist()\n\n        # Split into prompt + continuation\n        prompt_len = len(prompt_ids)\n        full_text = decode(generated_tokens)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:51:26.634193Z","iopub.execute_input":"2025-09-30T08:51:26.634898Z","iopub.status.idle":"2025-09-30T08:51:27.683869Z","shell.execute_reply.started":"2025-09-30T08:51:26.634872Z","shell.execute_reply":"2025-09-30T08:51:27.683128Z"}},"outputs":[{"name":"stdout","text":"Prompt: 17+19=?\nAnswer: ee ans ure estac+9uee eacawe ea7=?ere ec547=1 e ea eare e earesse eat estalst?eaat’mec25170 e eal30.\n\nPrompt: 3*17=?\nAnswer: ee anse There istatise ea15Bateee seatise ea925=atieseatien? eatiea32166 eac0 ea7 e e eac’s e 0255 eeale eals e ere e e e e ueal2155512112+5 eaeaeeeeeealls e e e e e es ec eal* Are 0.\n\nPrompt: 72/4=?\nAnswer: ee ansere istateese 3 eaIec6551=9=ueaec eal9.\n\nPrompt: 72-x=34,x=?\nAnswer: ee e ansureche 5c? uan\n\nPrompt: x*11=44,x=?\nAnswer: ee\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# eh???? trying original code below","metadata":{}},{"cell_type":"code","source":"!pip install matplotlib\n!pip install torch numpy transformers datasets tiktoken wandb tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:55:08.201655Z","iopub.execute_input":"2025-09-30T08:55:08.201909Z","iopub.status.idle":"2025-09-30T08:56:28.886614Z","shell.execute_reply.started":"2025-09-30T08:55:08.201886Z","shell.execute_reply":"2025-09-30T08:56:28.885928Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append(os.path.abspath(\"..\")) \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport pickle\n#from model import GPT, GPTConfig\nimport random\nfrom tqdm import tqdm\nimport time\nimport json\nimport matplotlib.pyplot as plt\n# Configuration\nbeta = 0.5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbase_lr = 1e-4\nepochs = 5\nbatch_size = 64\nmax_length =64\nnum_samples = 1\nmax_new_tokens = 200\ntemperature = 0.8\ntop_k = 200\n# tokenizer\nwith open(\"/kaggle/input/sft-meta/meta.pkl\", \"rb\") as f:\n    meta = pickle.load(f)\nstoi, itos = meta[\"stoi\"], meta[\"itos\"]\ndef encode(s): return [stoi[c] for c in s if c in stoi] # I EDITED THIS LINE\ndef decode(l): return ''.join([itos[i] for i in l])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:56:48.230765Z","iopub.execute_input":"2025-09-30T08:56:48.231530Z","iopub.status.idle":"2025-09-30T08:56:48.306547Z","shell.execute_reply.started":"2025-09-30T08:56:48.231494Z","shell.execute_reply":"2025-09-30T08:56:48.305849Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def compute_logprob(input_ids):\n    inputs = input_ids[:, :-1]\n    targets = input_ids[:, 1:]\n    logits, _ = gpt(inputs, full_seq=True)\n    B, T, V = logits.size()\n    logits_flat = logits.reshape(-1, V)\n    targets_flat = targets.reshape(-1)\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n    loss = loss.reshape(B, T)\n    attention_mask = (targets != 0).float()\n    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n    return -loss \n\ndef pad_or_truncate(seq, max_length):\n    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n\ndef get_batches(lines, batch_size):\n    random.shuffle(lines)\n    #for l in lines:\n    #    print(l[1])\n    for i in range(0, len(lines), batch_size):\n        batch = lines[i:i+batch_size]\n        if len(batch) < batch_size:\n            continue\n        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n        yield neg_tensor, pos_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:56:51.433563Z","iopub.execute_input":"2025-09-30T08:56:51.434052Z","iopub.status.idle":"2025-09-30T08:56:51.446291Z","shell.execute_reply.started":"2025-09-30T08:56:51.434016Z","shell.execute_reply":"2025-09-30T08:56:51.445640Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"ckpt = torch.load(\"/kaggle/input/sft-gpt/gpt.pt\", map_location=device)\ngptconf = GPTConfig(**ckpt['model_args'])\ngpt = GPT(gptconf)\nstate_dict = ckpt['model']\nunwanted_prefix = '_orig_mod.'\nfor k in list(state_dict.keys()):\n    if k.startswith(unwanted_prefix):\n        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\ngpt.load_state_dict(state_dict)\ngpt.to(device).train()\n\n\n### KIERAN ADDED THIS, REMEBER TO REMOVE BEFORE SUBMITTING\nprint(torch.cuda.is_available())\n\nprint(\"device variable:\", device)\nprint(\"Model first parameter device:\", next(gpt.parameters()).device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:04.867731Z","iopub.execute_input":"2025-09-30T08:57:04.868023Z","iopub.status.idle":"2025-09-30T08:57:05.202117Z","shell.execute_reply.started":"2025-09-30T08:57:04.868003Z","shell.execute_reply":"2025-09-30T08:57:05.201324Z"}},"outputs":[{"name":"stdout","text":"True\ndevice variable: cuda\nModel first parameter device: cuda:0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load data from ./data/pos_neg_pairs.json\n\n# Loading the json file, CHANGE ADDRESS IF NEEDED\nwith open(\"/kaggle/input/pos-neg-pairs/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n    lines = json.load(f)\n\nprint(f\"Loaded {len(lines)} pairs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:24.788569Z","iopub.execute_input":"2025-09-30T08:57:24.789156Z","iopub.status.idle":"2025-09-30T08:57:24.949136Z","shell.execute_reply.started":"2025-09-30T08:57:24.789133Z","shell.execute_reply":"2025-09-30T08:57:24.948441Z"}},"outputs":[{"name":"stdout","text":"Loaded 100000 pairs.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# recommend to use the AdamW optimizer \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nlearning_rate = 0.001\nweight_decay = 0.01 # This is the L2 regularization strength for AdamW\noptimizer = optim.AdamW(gpt.parameters(), lr=learning_rate, weight_decay=weight_decay)  \nscheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # Example: StepLR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:31.318498Z","iopub.execute_input":"2025-09-30T08:57:31.319216Z","iopub.status.idle":"2025-09-30T08:57:33.952288Z","shell.execute_reply.started":"2025-09-30T08:57:31.319191Z","shell.execute_reply":"2025-09-30T08:57:33.951502Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"total_steps = len(lines) // batch_size\nfor epoch in range(epochs):\n    pbar = tqdm(get_batches(lines, batch_size))\n    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n        ###########################################################\n        # Please complete the training code here!\n\n        # We first zero the gradients to avoid accumulation so that we can correctly compute the gradients for this step\n        optimizer.zero_grad()\n\n        # We calculate the log-probabilities\n        neg_logprob = compute_logprob(neg_tensor)\n        pos_logprob = compute_logprob(pos_tensor)\n\n        # We then calculate the loss of the DPO by the formula where we take the mean of the individual losses\n        loss = -F.logsigmoid((pos_logprob - neg_logprob) * beta).mean()\n\n        # We then backpropagate the loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # We update the progress bar with the current epoch, step, and loss\n        pbar.set_description(f\"Epoch {epoch + 1} Step {step + 1} Loss {loss.item():.4f}\")\n        ###########################################################\n        ckpt_path = f\"./dpo.pt\"\n        torch.save({\n            \"model_state_dict\": gpt.state_dict(),\n            \"model_args\": ckpt['model_args'],\n        }, ckpt_path)\n    print(f\"Saved checkpoint to {ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T08:57:40.143987Z","iopub.execute_input":"2025-09-30T08:57:40.144626Z","iopub.status.idle":"2025-09-30T09:28:34.058578Z","shell.execute_reply.started":"2025-09-30T08:57:40.144604Z","shell.execute_reply":"2025-09-30T09:28:34.057717Z"}},"outputs":[{"name":"stderr","text":"Epoch 1 Step 1562 Loss 0.0018: : 1562it [06:07,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Step 1562 Loss 0.0019: : 1562it [06:15,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Step 1562 Loss 0.0018: : 1562it [06:06,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Step 1562 Loss 0.0019: : 1562it [06:04,  4.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Step 1562 Loss 0.0019: : 1562it [06:20,  4.10it/s]","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint to ./dpo.pt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Test\ngpt.eval()\ntest_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\"]\n\nwith torch.no_grad():\n    for prompt in test_set:\n        # Encode text → tensor\n        prompt_ids = encode(prompt)\n        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n\n        # Generate continuation\n        out = gpt.generate(\n            x,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k\n        )\n\n        # Convert back to text\n        generated_tokens = out[0][0].cpu().tolist()\n\n        # Split into prompt + continuation\n        prompt_len = len(prompt_ids)\n        full_text = decode(generated_tokens)\n        continuation = decode(generated_tokens[prompt_len:])\n\n        print(f\"Prompt: {prompt}\")\n        print(f\"Answer: {continuation.strip()}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T09:28:34.060316Z","iopub.execute_input":"2025-09-30T09:28:34.061011Z","iopub.status.idle":"2025-09-30T09:28:35.886061Z","shell.execute_reply.started":"2025-09-30T09:28:34.060976Z","shell.execute_reply":"2025-09-30T09:28:35.885445Z"}},"outputs":[{"name":"stdout","text":"Prompt: 17+19=?\nAnswer: eau 5laaaall uauu0u   ucu 1sa  uwu1ihac3a 68 1 a   1  ca 3aau 1uu5 2uuiie0 lu4ai a uiau  aaa2tath uu.\n\nPrompt: 3*17=?\nAnswer: ecaaa1icaauuu7 u1icabct8auuw  uaa?uaauuu aaa  8u4a uui   cb aua8a a -lc2aca5uuubui aa5 s5q  451cu5a   uu7651ahi ucc1  au auacau aa    uh 5au   au u-ea a5    6e l a4e 1Xe a1ec u   ac  c39e1e l c’u0e7e1\n\nPrompt: 72/4=?\nAnswer: eaaanbnuui uau aaa u u a1     u  ualbl  u uu  1u  a\n\nPrompt: 72-x=34,x=?\nAnswer: ei ui natuaau5au+u 78u21a aa iaua4 u  a 9u1aata7 as  u211eu1ucwa  -116aa.\n\nPrompt: x*11=44,x=?\nAnswer: ellaaahaa a uucucau aa u.\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}