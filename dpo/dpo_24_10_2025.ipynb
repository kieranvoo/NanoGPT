{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "124a869a",
      "metadata": {
        "id": "124a869a"
      },
      "source": [
        "### Step 1: Install necesscary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3b82f8f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b82f8f1",
        "outputId": "27e96cb6-1111-4e84-c31a-3aeec7c0e3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: transformers in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.56.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.1.1)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.0)\n",
            "Requirement already satisfied: wandb in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.22.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.35.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.3.2)\n",
            "Requirement already satisfied: xxhash in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: click>=8.0.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (6.32.1)\n",
            "Requirement already satisfied: pydantic<3 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.11.9)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.39.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kiera\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kiera\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib\n",
        "%pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cCJJQXDZxqIV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCJJQXDZxqIV",
        "outputId": "9dedb588-0c9c-412a-d022-bbe1505a8951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Using device: cuda\n",
            "GPU name: NVIDIA GeForce RTX 3060 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# Check which device is being used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Optional: print GPU name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2d9de0",
      "metadata": {
        "id": "6c2d9de0"
      },
      "source": [
        "### Step 2: Package imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "876dd92d",
      "metadata": {
        "id": "876dd92d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from model import GPT, GPTConfig\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "# Configuration\n",
        "beta = 0.5\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "base_lr = 1e-4 #default is 1e-4\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "max_length =64\n",
        "num_samples = 1\n",
        "max_new_tokens = 200\n",
        "temperature = 0.7 # default is 0.8\n",
        "top_k = 200\n",
        "# tokenizer\n",
        "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "#def encode(s): return [stoi[c] for c in s]\n",
        "#def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = stoi.get(\"<unk>\", stoi.get(\" \", PAD_IDX))  # prefer <unk>, then space, else pad(0)\n",
        "\n",
        "def encode(s: str):\n",
        "    # map unseen characters to UNK instead of raising KeyError\n",
        "    return [stoi.get(c, UNK_IDX) for c in s]\n",
        "\n",
        "def decode(ids):\n",
        "    return ''.join(itos[i] for i in ids if 0 <= i < len(itos))\n",
        "\n",
        "# this the default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7d35e6",
      "metadata": {
        "id": "4c7d35e6"
      },
      "source": [
        "### Step 3: Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d03655c3",
      "metadata": {
        "id": "d03655c3"
      },
      "outputs": [],
      "source": [
        "def compute_logprob(input_ids):\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    logits, _ = gpt(inputs, full_seq=True)\n",
        "    B, T, V = logits.size()\n",
        "    logits_flat = logits.reshape(-1, V)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
        "    loss = loss.reshape(B, T)\n",
        "    attention_mask = (targets != 0).float()\n",
        "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    return -loss\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def get_batches(lines, batch_size):\n",
        "    random.shuffle(lines)\n",
        "    #for l in lines:\n",
        "    #    print(l[1])\n",
        "    for i in range(0, len(lines), batch_size):\n",
        "        batch = lines[i:i+batch_size]\n",
        "        if len(batch) < batch_size:\n",
        "            continue\n",
        "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
        "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
        "        yield neg_tensor, pos_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d9eba",
      "metadata": {
        "id": "fc9d9eba"
      },
      "source": [
        "### Step 4: Load the pretrained NanoGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ceae772a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae772a",
        "outputId": "3cb6400f-4ba8-4de3-a8c0-55ef164220e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(74, 348)\n",
              "    (wpe): Embedding(256, 348)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
              "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
        "gptconf = GPTConfig(**ckpt['model_args'])\n",
        "gpt = GPT(gptconf)\n",
        "state_dict = ckpt['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feafc5a",
      "metadata": {
        "id": "0feafc5a"
      },
      "source": [
        "### Step 5: Load Data (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7edf3d44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edf3d44",
        "outputId": "b19a0d56-514c-4d96-a459-365ad5306fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100000 pairs.\n"
          ]
        }
      ],
      "source": [
        "# Load data from pos_neg_pairs.json\n",
        "import json\n",
        "import tiktoken\n",
        "# Loading the json file, CHANGE ADDRESS IF NEEDED\n",
        "lines = \"\"\n",
        "with open(\"../pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n",
        " lines = json.load(f)\n",
        " print(f\"Loaded {len(lines)} pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRvKXw6NbG_R",
      "metadata": {
        "id": "QRvKXw6NbG_R"
      },
      "source": [
        "Step 6: Optimizer and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "xGyJrJX-5u62",
      "metadata": {
        "id": "xGyJrJX-5u62"
      },
      "outputs": [],
      "source": [
        "grad_clip = 1.0  # Gradient clipping to prevent explosion\n",
        "anchor_weight_start = 0.2\n",
        "anchor_weight_end = 0.05\n",
        "neg_anchor_weight = 0.001\n",
        "\n",
        "total_steps = len(lines) // batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "nuAj-SrEbJ_v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuAj-SrEbJ_v",
        "outputId": "1d4352b2-be66-44be-9759-be02c7de1ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 8,834,328 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,524 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "# AdamW optimizer\n",
        "import math\n",
        "optimizer = gpt.configure_optimizers(\n",
        "    weight_decay=0.01,            # consider 0.01 instead of 0.1\n",
        "    learning_rate=base_lr,        # e.g., 1e-3 or 1e-4\n",
        "    betas=(0.9, 0.95),            # your choice\n",
        "    device_type='cuda' if device=='cuda' else 'cpu'\n",
        ")\n",
        "\n",
        "# 2) Define a multiplicative factor schedule: warmup (0→1), then cosine to 0.1\n",
        "max_iters   = (len(lines) // batch_size) * epochs\n",
        "warmup_steps = max(1, int(0.03 * max_iters))\n",
        "\n",
        "def lr_factor(step: int):\n",
        "    if step < warmup_steps:\n",
        "        return (step + 1) / warmup_steps                  # 0→1 over warmup\n",
        "    t = (step - warmup_steps) / max(1, (max_iters - warmup_steps))\n",
        "    return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * t))  # decay to 0.1\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_factor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b66199",
      "metadata": {
        "id": "52b66199"
      },
      "source": [
        "### Step 7: Begin training (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26335eb",
      "metadata": {},
      "source": [
        "This one works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "855af817",
      "metadata": {},
      "outputs": [],
      "source": [
        "# total_steps = len(lines) // batch_size\n",
        "# for epoch in range(epochs):\n",
        "#     pbar = tqdm(get_batches(lines, batch_size))\n",
        "#     for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
        "#         ##########################################################################\n",
        "#         neg_log = compute_logprob(neg_tensor)\n",
        "#         pos_log = compute_logprob(pos_tensor)\n",
        "#         loss = -F.logsigmoid((pos_log - neg_log) / beta).mean() - 0.1 * pos_log.mean()\n",
        "\n",
        "#         global_step = epoch * total_steps + step + 1\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         lr = warmup_cosine_scheduler(total_steps * epochs, base_lr, global_step)\n",
        "\n",
        "#         for param_group in optimizer.param_groups:\n",
        "#             param_group['lr'] = lr\n",
        "\n",
        "#         optimizer.step()\n",
        "#         pbar.set_description(f\"epoch {epoch+1} step {step+1} loss={loss.item():.4f} lr={lr:.2e}\")\n",
        "#         ##########################################################################\n",
        "\n",
        "#     ckpt_path = f\"./dpo_epoch_{epoch+1}.pt\"\n",
        "#     torch.save({\n",
        "#         \"model_state_dict\": gpt.state_dict(),\n",
        "#         \"model_args\": ckpt['model_args'],\n",
        "#     }, ckpt_path)\n",
        "#     print(f\"Saved checkpoint to {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c3e621",
      "metadata": {},
      "source": [
        "This one doesnt work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d4ebeb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4ebeb4",
        "outputId": "dc13c51b-0e30-477e-e5c7-1cfd11c5a097"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(epochs):\n",
        "#     pbar = tqdm(get_batches(lines, batch_size))\n",
        "#     epoch_loss = 0.0  # Initialize the epoch loss for each epoch\n",
        "#     for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
        "#         ##########################################################################\n",
        "#         # Forward pass for negative and positive tensors\n",
        "#         neg_logprob = compute_logprob(neg_tensor)\n",
        "#         pos_logprob = compute_logprob(pos_tensor)\n",
        "\n",
        "#         # Compute the preference term (contrastive loss)\n",
        "#         logit_diff = (pos_logprob - neg_logprob) / beta\n",
        "#         preference_term = -torch.nn.functional.logsigmoid(logit_diff).mean()\n",
        "\n",
        "#         # Adaptive anchor term (adjust the importance of positive/negative samples)\n",
        "#         progress = epoch * total_steps + step  # Track progress over the entire training\n",
        "#         anchor_weight = anchor_weight_start * (1 - progress / (total_steps * epochs)) + anchor_weight_end * (progress / (total_steps * epochs))\n",
        "\n",
        "#         # Dual anchoring: encourage good positives, discourage negatives\n",
        "#         pos_anchor = -anchor_weight * pos_logprob.mean()\n",
        "#         neg_anchor = neg_anchor_weight * neg_logprob.mean()\n",
        "#         anchor_term = pos_anchor + neg_anchor\n",
        "\n",
        "#         # Total loss (contrastive + anchor regularization)\n",
        "#         loss = preference_term + anchor_term\n",
        "\n",
        "#         # Backward pass\n",
        "#         optimizer.zero_grad()  # Zero the gradients before backpropagation\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Clip gradients for stability\n",
        "#         torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
        "\n",
        "#         # Optimizer step: update model parameters\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Track loss for the epoch\n",
        "#         epoch_loss += loss.item()\n",
        "\n",
        "#         # Update progress bar description with loss and learning rate info\n",
        "#         lr = scheduler.get_last_lr()[0]  # Get the current learning rate from scheduler\n",
        "#         pbar.set_description(f\"Epoch {epoch + 1}/{epochs} | Step {step}/{total_steps} | Loss {loss.item():.4f} | LR {lr:.2e}\")\n",
        "\n",
        "#     # Update the learning rate scheduler at the end of each epoch\n",
        "#     scheduler.step()\n",
        "\n",
        "#     # Print average loss for the epoch\n",
        "#     print(f\"Epoch {epoch + 1} completed. Avg Loss: {epoch_loss / total_steps:.4f}\")\n",
        "#         ##########################################################################\n",
        "\n",
        "#     ckpt_path = f\"./dpo.pt\"\n",
        "#     torch.save({\n",
        "#             \"model_state_dict\": gpt.state_dict(),\n",
        "#             \"model_args\": ckpt['model_args'],\n",
        "#         }, ckpt_path)\n",
        "#     print(f\"Saved checkpoint to {ckpt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa4ad3e",
      "metadata": {},
      "source": [
        "Kieran's new try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dcdb4886",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Step 1562/1562 | Loss -0.0237 | LR 9.33e-05: : 1562it [02:38,  9.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Avg Loss: 0.1708\n",
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 | Step 1562/1562 | Loss -0.1173 | LR 7.14e-05: : 1562it [02:38,  9.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. Avg Loss: -0.0707\n",
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 | Step 1562/1562 | Loss -0.1945 | LR 4.28e-05: : 1562it [02:37,  9.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed. Avg Loss: -0.1573\n",
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 | Step 1562/1562 | Loss -0.2428 | LR 1.91e-05: : 1562it [02:36,  9.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 completed. Avg Loss: -0.2188\n",
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 | Step 1562/1562 | Loss -0.2623 | LR 1.00e-05: : 1562it [02:36,  9.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 completed. Avg Loss: -0.2529\n",
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "global_step = 0\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(get_batches(lines, batch_size))\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
        "        # forward\n",
        "        neg_logprob = compute_logprob(neg_tensor)\n",
        "        pos_logprob = compute_logprob(pos_tensor)\n",
        "        logit_diff = (pos_logprob - neg_logprob) / beta\n",
        "        preference_term = -torch.nn.functional.logsigmoid(logit_diff).mean()\n",
        "\n",
        "        progress = epoch * total_steps + step\n",
        "        anchor_weight = anchor_weight_start * (1 - progress / (total_steps * epochs)) \\\n",
        "                        + anchor_weight_end   * (progress / (total_steps * epochs))\n",
        "        pos_anchor = -anchor_weight * pos_logprob.mean()\n",
        "        neg_anchor =  neg_anchor_weight * neg_logprob.mean()\n",
        "        loss = preference_term + (pos_anchor + neg_anchor)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()                 # <-- per-iteration step\n",
        "        global_step += 1\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        current_lr = optimizer.param_groups[0]['lr']  # read actual LR\n",
        "        pbar.set_description(\n",
        "            f\"Epoch {epoch+1}/{epochs} | Step {step+1}/{total_steps} | \"\n",
        "            f\"Loss {loss.item():.4f} | LR {current_lr:.2e}\"\n",
        "        )\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} completed. Avg Loss: {epoch_loss / total_steps:.4f}\")\n",
        "\n",
        "    # save\n",
        "    torch.save({\"model_state_dict\": gpt.state_dict(),\n",
        "                \"model_args\": ckpt['model_args']}, \"./dpo.pt\")\n",
        "    print(\"Saved checkpoint to ./dpo.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b7f2ab",
      "metadata": {
        "id": "48b7f2ab"
      },
      "source": [
        "### Step 8: Begin testing (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "09027262",
      "metadata": {
        "id": "09027262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100+19=? The answer is 119 because 100+19 equals 119.\n",
            "3*17=? The answer is 57 because 3*17 equals 57.\n",
            "72/4=? The answer is 17 because 72//4 equals 17.\n",
            "72-x=34,x=? The answer is 40 because 72-40 equals 34.\n",
            "x*11=44,x=? The answer is 34 because 34+11 equals 44.\n",
            "3*17=? The answer is 67 because 3*17 equals 67.\n",
            "72/4=? The answer is 15 because 72//4 equals 15.\n",
            "72-x=34,x=? The answer is 38 because 72-38 equals 34.\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "ckpt_path = \"../dpo/dpo.pt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "gpt = GPT(gptconf).cuda()\n",
        "try:\n",
        "    state_dict = checkpoint['model']\n",
        "except:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "# Test\n",
        "gpt.eval()\n",
        "test_set = [\"100+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
        "with torch.no_grad():\n",
        "    for prompt in test_set:\n",
        "        prompt_ids = encode(prompt)\n",
        "        ###########################################################\n",
        "        prompts_ids_formatted = torch.tensor([prompt_ids],dtype=torch.long,device=device)\n",
        "        result = gpt.generate(prompts_ids_formatted, max_new_tokens, temperature, top_k)\n",
        "        decoded_result = decode(result[0].detach().cpu().view(-1).tolist())\n",
        "        print(decoded_result)\n",
        "        ###########################################################\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
