{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp310-cp310-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.2/8.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 13.2 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 14.3 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.6 pyparsing-3.2.5\n",
      "Requirement already satisfied: numpy in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (2.0.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.22.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting click>=8.0.1 (from wandb)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from wandb) (4.3.7)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.39.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl.metadata (76 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rkuru\\anaconda3\\envs\\pytorch_gpu_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading tiktoken-0.11.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 884.2/884.2 kB 13.2 MB/s  0:00:00\n",
      "Using cached wandb-0.22.1-py3-none-win_amd64.whl (18.8 MB)\n",
      "Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 15.4 MB/s  0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl (452 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/26.2 MB 4.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.4/26.2 MB 11.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/26.2 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/26.2 MB 12.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.5/26.2 MB 12.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.7/26.2 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.6/26.2 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.7/26.2 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.9/26.2 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 13.4 MB/s  0:00:02\n",
      "Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading sentry_sdk-2.39.0-py2.py3-none-any.whl (370 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 15.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.3 MB 15.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.3 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 14.2 MB/s  0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-inspection, tqdm, smmap, sentry-sdk, safetensors, regex, pydantic-core, pyarrow, protobuf, propcache, multidict, fsspec, frozenlist, dill, click, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, tiktoken, pydantic, pandas, multiprocess, huggingface-hub, gitdb, aiosignal, tokenizers, gitpython, aiohttp, wandb, transformers, datasets\n",
      "\n",
      "   ----------------------------------------  0/36 [pytz]\n",
      "   -- -------------------------------------  2/36 [tzdata]\n",
      "   -- -------------------------------------  2/36 [tzdata]\n",
      "   ---- -----------------------------------  4/36 [tqdm]\n",
      "   ------ ---------------------------------  6/36 [sentry-sdk]\n",
      "   ------ ---------------------------------  6/36 [sentry-sdk]\n",
      "   -------- -------------------------------  8/36 [regex]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ----------- ---------------------------- 10/36 [pyarrow]\n",
      "   ------------ --------------------------- 11/36 [protobuf]\n",
      "   ------------- -------------------------- 12/36 [propcache]\n",
      "   --------------- ------------------------ 14/36 [fsspec]\n",
      "   ------------------ --------------------- 17/36 [click]\n",
      "   ------------------------ --------------- 22/36 [yarl]\n",
      "   -------------------------- ------------- 24/36 [pydantic]\n",
      "   -------------------------- ------------- 24/36 [pydantic]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   --------------------------- ------------ 25/36 [pandas]\n",
      "   ---------------------------- ----------- 26/36 [multiprocess]\n",
      "   ------------------------------ --------- 27/36 [huggingface-hub]\n",
      "   ------------------------------ --------- 27/36 [huggingface-hub]\n",
      "   ------------------------------- -------- 28/36 [gitdb]\n",
      "   ---------------------------------- ----- 31/36 [gitpython]\n",
      "   ----------------------------------- ---- 32/36 [aiohttp]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------ --- 33/36 [wandb]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   ------------------------------------- -- 34/36 [transformers]\n",
      "   -------------------------------------- - 35/36 [datasets]\n",
      "   -------------------------------------- - 35/36 [datasets]\n",
      "   ---------------------------------------- 36/36 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 async-timeout-5.0.1 attrs-25.3.0 click-8.3.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 fsspec-2025.9.0 gitdb-4.0.12 gitpython-3.1.45 huggingface-hub-0.35.3 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.3 propcache-0.3.2 protobuf-6.32.1 pyarrow-21.0.0 pydantic-2.11.9 pydantic-core-2.33.2 pytz-2025.2 regex-2025.9.18 safetensors-0.6.2 sentry-sdk-2.39.0 smmap-5.0.2 tiktoken-0.11.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 typing-inspection-0.4.1 tzdata-2025.2 wandb-0.22.1 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1 requires sympy==1.13.1, but you have sympy 1.14.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "# !pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "!pip install numpy transformers datasets tiktoken wandb tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# tokenizer\n",
    "# # tokenizer\n",
    "# with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "#     meta = pickle.load(f)\n",
    "# stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "# #def encode(s): return [stoi[c] for c in s]\n",
    "# #def decode(l): return ''.join([itos[i] for i in l])\n",
    "# PAD_IDX = 0\n",
    "# UNK_IDX = stoi.get(\"<unk>\", stoi.get(\" \", PAD_IDX))  # prefer <unk>, then space, else pad(0)\n",
    "\n",
    "# def encode(s: str):\n",
    "#     # map unseen characters to UNK instead of raising KeyError\n",
    "#     return [stoi.get(c, UNK_IDX) for c in s]\n",
    "\n",
    "# def decode(ids):\n",
    "#     return ''.join(itos[i] for i in ids if 0 <= i < len(itos))\n",
    "# with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "#     meta = pickle.load(f)\n",
    "# stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "# def encode(s): return [stoi[c] for c in s if c in stoi] # I EDITED THIS LINE\n",
    "# def decode(l): return ''.join([itos[i] for i in l])\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "#def encode(s): return [stoi[c] for c in s]\n",
    "#def decode(l): return ''.join([itos[i] for i in l])\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = stoi.get(\"<unk>\", stoi.get(\" \", PAD_IDX))  # prefer <unk>, then space, else pad(0)\n",
    "\n",
    "def encode(s: str):\n",
    "    # map unseen characters to UNK instead of raising KeyError\n",
    "    return [stoi.get(c, UNK_IDX) for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(itos[i] for i in ids if 0 <= i < len(itos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)   \n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkuru\\AppData\\Local\\Temp\\ipykernel_34224\\3578242387.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1212 12.1\n",
      "cuda\n",
      "True\n",
      "Model first parameter device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()\n",
    "\n",
    "\n",
    "# ### KIERAN ADDED THIS, REMEBER TO REMOVE BEFORE SUBMITTING\n",
    "print(torch.cuda.is_available())\n",
    "import torch\n",
    "print(1212,torch.version.cuda)\n",
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Model first parameter device:\", next(gpt.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pairs.\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "import json\n",
    "import tiktoken\n",
    "# Loading the json file, CHANGE ADDRESS IF NEEDED\n",
    "with open(\"../dpo/pos_neg_pairs.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(lines)} pairs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import torch\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# # ---------- AdamW with proper weight decay ----------\n",
    "# wd = 0.1            # weight decay for weights (not biases/norm/embeddings)\n",
    "# base_lr = 1e-4      # your chosen base LR\n",
    "# betas = (0.9, 0.95)\n",
    "# eps = 1e-8\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# decay, no_decay = [], []\n",
    "# for n, p in gpt.named_parameters():\n",
    "#     if not p.requires_grad:\n",
    "#         continue\n",
    "#     is_bias = n.endswith(\".bias\")\n",
    "#     is_norm = (\"norm\" in n.lower()) or (\"ln\" in n.lower())\n",
    "#     is_embed = \"embed\" in n.lower()\n",
    "#     if is_bias or is_norm or is_embed:\n",
    "#         no_decay.append(p)\n",
    "#     else:\n",
    "#         decay.append(p)\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     [\n",
    "#         {\"params\": decay, \"weight_decay\": wd},\n",
    "#         {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "#     ],\n",
    "#     lr=base_lr,\n",
    "#     betas=betas,\n",
    "#     eps=eps,\n",
    "# )\n",
    "\n",
    "# # ---------- Warmup + Cosine scheduler ----------\n",
    "# steps_per_epoch = len([1 for _ in range(0, len(lines), batch_size)])  # or len(lines)//batch_size\n",
    "# total_steps = epochs * steps_per_epoch\n",
    "# warmup_steps = max(1, int(0.1 * total_steps))  # 10% warmup (tweak as you like)\n",
    "\n",
    "# def lr_lambda(step):\n",
    "#     if step < warmup_steps:\n",
    "#         return float(step) / float(max(1, warmup_steps))\n",
    "#     # cosine decay from 1.0 -> 0.0 over the remaining steps\n",
    "#     progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "#     return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "# print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "\n",
    "\n",
    "# THIS IS THEIRS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "import math \n",
    "weight_decay = 1e-3\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "max_iters = (len(lines) // batch_size) * epochs\n",
    "warmup_iters =  int(0.1 * max_iters)\n",
    "\n",
    "lr_decay_iters = max_iters\n",
    "base_lr =  6e-4\n",
    "min_lr = base_lr / 10\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in gpt.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # Don't apply weight decay to biases and layer norms\n",
    "        if 'bias' in name or 'ln' in name or 'layernorm' in name:\n",
    "            no_decay_params.append(param)\n",
    "        else:\n",
    "            decay_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params, 'weight_decay': 1e-2},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "], lr=base_lr, betas=(beta1, beta2))\n",
    "\n",
    "# Learning rate decay scheduler (cosine with warmup) \n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return base_lr * (it + 1) / (warmup_iters + 1)\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (base_lr - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1355cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can implement mixed precision training here (also implemented in the nanogpt train.py, i copied code here)\n",
    "\n",
    "# \"\"\"\n",
    "# What is Mixed Precision Training? (sorry copied from chatgpt but i thinkk it makes sense?? it sped up the training time considerably)\n",
    "\n",
    "# Normally, training uses 32‑bit floats (FP32) for everything.\n",
    "# Mixed precision means:\n",
    "# - Use 16‑bit floats (FP16 or BF16) for most forward & backward ops (faster + less memory).\n",
    "# - Keep a few things in 32‑bit to avoid numerical instability (like the master weights in optimizer).\n",
    "# - This gives ~2–3× training speedups on GPUs and lets you fit bigger batches.\n",
    "# \"\"\"\n",
    "# from contextlib import nullcontext\n",
    "\n",
    "# device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "# # use bf16 if the gpu supports it, else fall back to fp16\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "# ptDtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "# # float32 (full precision, 32-bit): default in pytorch, stable, but memory-heavy and slower on gpus\n",
    "# # float16 / bfloat16 (half precision, 16-bit): uses half the memory, runs way faster on gpus\n",
    "# # - but downside: less numerical precision, risk of underflow / overflow\n",
    "\n",
    "# # hence: amp (automatic mixed precision) : use half precision when possible + fallback to float32 when needed\n",
    "# # - matrix multiplication -> good in float16 (fast on gpu)\n",
    "# # - loss, reductions -> keep in float32 (stable)\n",
    "# # (dynamically decided per operation)\n",
    "# ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptDtype)\n",
    "# # Initialize GradScaler\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# # special case: tf32 settings (not a precision type explicitly picked, more of a backend optimisation)\n",
    "# # these flags affect matrix multiplication and convolutions\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "# torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "# # if running on fp32 outcast, python is allowed to use tf32 internally\n",
    "\n",
    "\n",
    "# # fp16 leads to gradient underflow\n",
    "# # fp16 has a much smaller range than fp32\n",
    "# # - fp32 can represent numbers as small as ~10^-38\n",
    "# # - fp16: can represent numbers as small as ~6x10^-8\n",
    "# # during backpropogation, gradients get very small. in fp16, these tiny gradients underflow to 0 -> model stops learning\n",
    "\n",
    "# # solution: dynamic loss scaling using gradscaler\n",
    "# # gradscaler cycle:\n",
    "# # - scaler.scale(loss) → multiply loss by a scale factor (e.g., 1024, 2048)\n",
    "# # - .backward() → gradients are now scaled up (less likely to underflow)\n",
    "# # - scaler.unscale_(optimizer) → divide gradients back down to their true values\n",
    "# # - scaler.step(optimizer) → optimizer uses the unscaled gradients\n",
    "# # - scaler.update() → adjust the scale factor for next iteration\n",
    "# scaler = torch.cuda.amp.GradScaler('cuda',enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bc9913cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Step 1561 | Loss 0.0000 | LR 9.31e-05: : 1562it [02:03, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Step 3123 | Loss 0.0000 | LR 7.10e-05: : 1562it [02:03, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Step 4685 | Loss 0.0000 | LR 4.25e-05: : 1562it [02:03, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Step 6247 | Loss 0.0000 | LR 1.90e-05: : 1562it [02:03, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Step 7809 | Loss 0.0000 | LR 1.00e-05: : 1562it [02:03, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# total_steps = len(lines) // batch_size\n",
    "# global_step = 0\n",
    "# for epoch in range(epochs):\n",
    "#     pbar = tqdm(get_batches(lines, batch_size))\n",
    "#     for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "#         ###########################################################\n",
    "\n",
    "#         # We first zero the gradients to avoid accumulation so that we can correctly compute the gradients for this step\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # We calculate the log-probabilities\n",
    "#         neg_logprob = compute_logprob(neg_tensor)\n",
    "#         pos_logprob = compute_logprob(pos_tensor)\n",
    "\n",
    "#         # We then calculate the loss of the DPO by the formula where we take the mean of the individual losses\n",
    "#         loss = -F.logsigmoid((pos_logprob - neg_logprob) * beta).mean()\n",
    "        \n",
    "#         # We then backpropagate the loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "\n",
    "#         lr = get_lr(global_step)\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "#         optimizer.step()\n",
    "#         global_step += 1\n",
    "        \n",
    "\n",
    "#         # We update the progress bar with the current epoch, step, and loss\n",
    "#         pbar.set_description(f\"Epoch {epoch + 1} Step {step + 1} Loss {loss.item():.4f}\")\n",
    "#         ###########################################################\n",
    "#     ckpt_path = f\"./dpo.pt\"\n",
    "#     torch.save({\n",
    "#         \"model_state_dict\": gpt.state_dict(),\n",
    "#         \"model_args\": ckpt['model_args'],\n",
    "#     }, ckpt_path)\n",
    "#     print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) * beta).mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), grad_clip)\n",
    "        \n",
    "        # Update learning rate BEFORE optimizer step\n",
    "        lr = get_lr(global_step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{epochs} | Step {global_step} | Loss {loss.item():.4f} | LR {lr:.2e}\")\n",
    "        \n",
    "        # Increment global step counter\n",
    "        global_step += 1\n",
    "    \n",
    "    # Save checkpoint ONCE per epoch\n",
    "    ckpt_path = f\"./dpo_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b613a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt.train()\n",
    "# total_steps = len(lines) // batch_size\n",
    "# for epoch in range(epochs):\n",
    "#     pbar = tqdm(get_batches(lines, batch_size))\n",
    "#     for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "#         ###########################################################\n",
    "#         # ---- forward: log-probs for positive and negative samples ----\n",
    "#         pos_logprob = compute_logprob(pos_tensor)   # [B]\n",
    "#         neg_logprob = compute_logprob(neg_tensor)   # [B]\n",
    "\n",
    "#         # ---- loss: preference-style (DPO-ish) + small anchor on positives ----\n",
    "#         preference_term = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n",
    "#         anchor_term      = -0.10 * pos_logprob.mean()   # optional; set to 0.0 to disable\n",
    "#         loss = preference_term + anchor_term\n",
    "\n",
    "#         # ---- backward + step ----\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(gpt.parameters(), max_grad_norm)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # ---- progress bar ----\n",
    "#         if (step + 1) % 50 == 0:\n",
    "#             pbar.set_postfix({\n",
    "#                 \"loss\": f\"{loss.item():.4f}\",\n",
    "#                 \"pref\": f\"{preference_term.item():.4f}\",\n",
    "#                 \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "#             })\n",
    "#         ###########################################################\n",
    "#     ckpt_path = f\"./dpo.pt\"\n",
    "#     torch.save({\n",
    "#         \"model_state_dict\": gpt.state_dict(),\n",
    "#         \"model_args\": ckpt['model_args'],\n",
    "#     }, ckpt_path)\n",
    "#     print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "\n",
    "# THIS I STHIERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkuru\\AppData\\Local\\Temp\\ipykernel_34224\\1280974629.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Answer: eee e anZe The is ooe -8   tkequnZ oeZ oedZ 106 oedZ  oequrej oequ onZ onZ oedZ oe,edZ -Z oedZ 6Z oe,edZ +Z 9*-2+Zre,e?Z oe,eredZ oe,edZ oe,e,e,e onZ oedZ onZ oewedZ onZ onZ owe,e,e onZ owe180=Z oe,e,\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Answer: eee e anZe is yea 9 uoe oedZ -97 e,e5 equredZ   equreqoonZ  onZre,edZ 90-6Z oe oedZ onZ oe,e,e.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Answer: eee e oectnZ seca se 14  oequaure  oequ/1160.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Answer: eeee anZe TIej  eq oe oequanZ  oeqonZ /+1*7.\n",
      "\n",
      "Prompt: x*11=44,x=?\n",
      "Answer: eeee anZe is 5 equoe te 80+9 equals 111  11.\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Answer: eee e anZe keq ooe oe0Z oedZ re oequalnZ 1+Z oeredZ onZ oe,e onZ oe,e.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Answer: eee e T tnZ tec sequnZ  oedZ 1-1 oe,equredZ oeq0.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Answer: eeee anZe TIev oere is 6=Z equanZ 9.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo_epoch_5.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # This part i gpt generated could be wrong, couldnt find this in train.py lol \n",
    "        # Encode text → tensor\n",
    "        prompt_ids = encode(prompt)\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, len(prompt)]\n",
    "\n",
    "        # Generate continuation\n",
    "        out = gpt.generate(\n",
    "            x,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Convert back to text\n",
    "        generated_tokens = out[0][0].cpu().tolist()\n",
    "\n",
    "        # Split into prompt + continuation\n",
    "        prompt_len = len(prompt_ids)\n",
    "        full_text = decode(generated_tokens)\n",
    "        continuation = decode(generated_tokens[prompt_len:])\n",
    "\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Answer: {continuation.strip()}\\n\")\n",
    "        ###########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
